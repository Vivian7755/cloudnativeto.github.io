[{"categories":["Kubernetes"],"contents":" 前言 这个问题 flannel 和 calico 的 VXLAN 模式下都会发生，部分人的集群的A记录 UDP 下查询可能有问题。原因是 v1.17+ 的 kubernetes 某部分会引起内核的某个 UDP 相关的 BUG 而不是 CNI 的软件层面， WEAVE 没有这个问题，原因后面会说到。写这篇文章的日期是05/28，最开始发现是上周五也就是05/23号，文章从时间线写起，因为很多时候想发文章但是没空。\n由来 上周五我经过同事的工位看到同事的桌面是 kubectl get po 的输出，问他咋开始学 Kubernetes 了，他说跟着视频学下。看了下用的 kubeadm 部署了一套1.18.2的集群。1.18的 kube-proxy 的 ipvs 包的 parseIP 有 bug ，我推荐他换v1.17.5。他当时在部署一个入门的 SVC 实验，无法解析域名。使用dig命令排查了下，下面是对照:\n dig @\u0026lt;podIP\u0026gt; +short kubernetes.default.svc.cluster.local 能解析 dig @10.96.0.10 +short kubernetes.default.svc.cluster.local 超时  很多市面上的kubeadm部署教程都是直接命令 kubeadm init 的，所以我推荐同事去按照我文章的 kubeadm部署 一套后再试试，叫他用v1.17的最新版本v1.17.5，结果还是上面一样。 coredns 实际上还有 metrics 的 http 接口，从 http 层测了下：\n curl -I 10.96.0.10:9153/metrics 超时，很久之后才有返回 curl -I \u0026lt;podIP\u0026gt;:9153/metrics 能直接返回  涉及到本次排查的信息为：\n$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready master 7d8h v1.18.2 10.0.100.3 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://19.3.8 node1 Ready \u0026lt;none\u0026gt; 7d7h v1.18.2 10.0.100.4 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://19.3.8 node2 Ready \u0026lt;none\u0026gt; 7d7h v1.18.2 10.0.100.15 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://19.3.8 $ kubectl get po -o wide -n kube-system -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-546565776c-v5wwg 1/1 Running 2 25h 10.244.2.73 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  多次尝试发现很久的时间都是一样，用 time 命令观察了下一直是63秒返回。包括其他任何 SVC 都是这样。\n$ time curl -I 10.96.0.10:9153/metrics HTTP/1.1 200 OK Content-Type: text/plain; version=0.0.4; charset=utf-8 Date: Wed, 25 May 2020 08:39:35 GMT real\t1m3.091s user\t0m0.002s sys\t0m0.007s  proxyMode 是 ipvs ，用 ipvsadm 看下超时的时候的状态，一直是SYN_RECV，也就是发送了 SYN ，没收到回包。\n$ ipvsadm -lnc |\u0026amp; grep 9153 TCP 00:59 SYN_RECV 10.96.0.10:41282 10.96.0.10:9153 10.244.2.73:9153  抓包 因为 CNI 使用的 flannel ，用的 VXLAN 模式。master 上抓9153和flannel.1的 8472 端口，coredns 的 POD 所在 node 上抓 flannel 的 VXLAN 包，下面三个是对应的:\n[root@master /root]# tcpdump -nn -i flannel.1 port 9153 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes 16:30:56.705696 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17148909 ecr 0,nop,wscale 7], length 0 16:30:57.708489 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17149912 ecr 0,nop,wscale 7], length 0 16:30:59.712458 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17151916 ecr 0,nop,wscale 7], length 0 16:31:03.716441 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17155920 ecr 0,nop,wscale 7], length 0 16:31:11.732562 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17163936 ecr 0,nop,wscale 7], length 0 16:31:27.764498 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17179968 ecr 0,nop,wscale 7], length 0 16:31:59.828493 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0 16:31:59.829565 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0 16:31:59.829611 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0 16:31:59.829714 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87 16:31:59.829897 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0 16:31:59.831300 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112 16:31:59.831322 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0 16:31:59.831435 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0 16:31:59.831633 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0 16:31:59.831660 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0  [root@master /root]# tcpdump -nn -i eth0 port 8472 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:30:56.705718 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17148909 ecr 0,nop,wscale 7], length 0 16:30:57.708523 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17149912 ecr 0,nop,wscale 7], length 0 16:30:59.712478 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17151916 ecr 0,nop,wscale 7], length 0 16:31:03.716452 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17155920 ecr 0,nop,wscale 7], length 0 16:31:11.732590 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17163936 ecr 0,nop,wscale 7], length 0 16:31:27.764513 IP 10.0.100.3.48683 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17179968 ecr 0,nop,wscale 7], length 0 16:31:59.828541 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0 16:31:59.829521 IP 10.0.100.15.56771 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0 16:31:59.829617 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0 16:31:59.829729 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87 16:31:59.829883 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0 16:31:59.831292 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112 16:31:59.831327 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0 16:31:59.831448 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0 16:31:59.831612 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0 16:31:59.831665 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0  [root@node2 /root]# tcpdump -nn -i eth0 port 8472 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:31:59.836137 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0 16:31:59.836328 IP 10.0.100.15.56771 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0 16:31:59.836811 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0 16:31:59.836910 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87 16:31:59.836951 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0 16:31:59.838385 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112 16:31:59.838522 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0 16:31:59.838621 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0 16:31:59.838703 IP 10.0.100.15.34571 \u0026gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.2.73.9153 \u0026gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0 16:31:59.838836 IP 10.0.100.3.56618 \u0026gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1 IP 10.244.0.0.2201 \u0026gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0  先看上面的第一部分，搜了下资料，得知 TCP 默认 SYN 报文最大 retry 5次，每次超时了翻倍，1s -\u0026gt; 3s -\u0026gt; 7s -\u0026gt; 15s -\u0026gt; 31s -\u0026gt; 63s。只有63秒的时候 node 的机器上才收到了 VXLAN 的报文。说明 POD 所在 node 压根没收到63秒之前的。\n一般 LVS 的 dr 模式下 TCP 的时间戳混乱或者其他几个 ARP 的内核参数不对下 SYN 是一直收不到的而不是63秒后有结果，所以和内核相关参数无关。于是同样上面的步骤 tcpdump 抓包，加上-w filename.pcap选项把抓的包导出下来导入到 wireshark 里准备看看。\n报文分析 9153的包 wireshark 里看63秒前面都是 TCP 的 SYN 重传，看到了 master 上向外发送的 VXLAN 报文的时候有了发现。\n可以看到 UDP 的 checksum 是0xffff，我对 UDP 报文不太熟悉， UDP 的 header 的 Checksum 没记错的话CRC32校验的，不可能是这种两个字节都置1的 0xffff ，明显就是 UDP 的 header 的校验出错了。后面几个正常包的 Checksum 都是 missing 的。\nwireshark 的编辑-\u0026gt;首选项-\u0026gt;Protocols-\u0026gt;UDP-\u0026gt;Validate the UDP checksum if possible 勾上更直观看。\n不是根本的解决方法 搜了下wireshark linux udp checksum incorrect，都是推荐把 Checksum Offload disable 掉就行了，例如我这里是 flannel ，则是：\n$ /sbin/ethtool -K flannel.1 tx-checksum-ip-generic off Actual changes: tx-checksumming: off tx-checksum-ip-generic: off tcp-segmentation-offload: off tx-tcp-segmentation: off [requested on] tx-tcp-ecn-segmentation: off [requested on] tx-tcp6-segmentation: off [requested on] tx-tcp-mangleid-segmentation: off [requested on] udp-fragmentation-offload: off [requested on]  再测下正常，而 WEAVE 他们也用的 VXLAN 模式，但是他们在创建网卡的时候把这个已经 off 掉了，所以 WEAVE 的 VXLAN 模式在v1.17+集群没出现这个问题。\n$ time curl -I 10.96.0.10:9153 HTTP/1.1 404 Not Found Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Wed, 27 May 2020 02:14:04 GMT Content-Length: 19 real\t0m0.009s user\t0m0.005s sys\t0m0.003s  你以为这样就完了？其实并没有，因为我自己维护了一套 ansible 部署 kubernetes 的方案，每次新版本发布我都会实际测下。并且同事反映了他同样云主机开出来用我 ansible 部署v1.17.5没有这个问题。这就很奇怪了，原因后面说，请接着继续看。\n什么是checksum offload Checksum Offload 是网卡的一个功能选项。如果该选项开启，则网卡层面会计算需要发送或者接收到的消息的校验和，从而节省 CPU 的计算开销。此时，在需要发送的消息到达网卡前，系统会在报头的校验和字段填充一个随机值。但是，尽管校验和卸载能够降低 CPU 的计算开销，但受到计算能力的限制，某些环境下的一些网络卡计算速度不如主频超过 400MHz 的 CPU 快。\n正文 对照组 很奇怪的就是为啥就是我的 ansible 部署的二进制就正常没这个问题，而 kubeadm 部署的就不正常，后面我花时间整了以下几个对照组(期间同事也帮我做了几个条件下的测试，但是不是系统用错了就是版本整错了。。。)，终于找到了问题的范围，下面是我自己统计的对照组信息， kubeadm 和 ansible 版本均为1.17.5测试。os 不重要，因为最终排查出和 os 无关：\n   os type(kubeadm or ansible) flannel version flannel is running in pod? will 63 sec delay?     7.6 kubeadm v0.11.0 yes yes   7.6 kubeadm v0.12.0 yes yes   7.6 kubeadm v0.11.0 no yes   7.6 kubeadm v0.12.0 no yes   7.6 ansible v0.11.0 yes no   7.6 ansible v0.12.0 yes no   7.6 ansible v0.11.0 no no   7.6 ansible v0.12.0 no no    这就看起来很迷了。但是排查出和 flannel 无关，感觉 kube-proxy 有关系，然后今天05/28针对 kube-proxy 做了个对照组：\n   os type(kubeadm or ansible) kube-proxy version kube-proxy is running in pod? will 63 sec delay?     7.6 kubeadm v1.17.5 yes yes   7.6 kubeadm v1.17.5 no no   7.6 kubeadm v1.16.9 yes no   7.6 kubeadm v1.16.9 no no   7.6 ansible v1.17.5 yes yes   7.6 ansible v1.17.5 no no    可以看出就是1.17以上的 kube-proxy 如果使用 POD 则会有这个问题，而非 POD 则不会， 在github 上 compare 了下v1.17.0和v1.16.3。\n发现了 Dockerfile的改动 ， 1.17.0里的 Dockerfile 的BASEIMAGE是用 指定了一个源安装了最新的iptables，然后利用update-alternatives把脚本/usr/sbin/iptables-wrapper去替代iptables 来检测应该使用nft还是legacy， hack 下镜像回自带源里的 iptables 验证下。\nFROM registry.aliyuncs.com/google_containers/kube-proxy:v1.17.5 RUN rm -f /usr/sbin/iptables \u0026amp;\u0026amp; clean-install iptables  构建的镜像推送到了 dockerhub 上zhangguanzhang/hack-kube-proxy:v1.17.5，更改下集群 kube-proxy ds 的镜像。\n$ kubectl -n kube-system get ds kube-proxy -o yaml | grep image: image: zhangguanzhang/hack-kube-proxy:v1.17.5  测试访问成功。\n$ time curl -I 10.96.0.10:9153 HTTP/1.1 404 Not Found Content-Type: text/plain; charset=utf-8 X-Content-Type-Options: nosniff Date: Thu, 28 May 2020 04:47:21 GMT Content-Length: 19 real\t0m0.008s user\t0m0.003s sys\t0m0.003s  对于这个问题我在 flannel 的 pr 下面也参与了回复，同时在官方 github 上提了一个 issue。\n这个问题的触发是由于v1.17+的 kube-proxy 的 docker 镜像里安装了最新的 iptables ， --random-fully选项会触发内核vxlan的bug。\n总结 目前解决问题三种办法:\n 关闭 CNI 的 VXLAN 网卡的 checksum offload 更改 Docker 镜像 升级到新内核，具体版本就不知道了，只要在这个 内核pr 合并后出的内核版本都行，有人说这些可以 Stable kernels 5.6.13, 5.4.41, 4.19.123, 4.14.181 and later have the checksum patch included.  参考链接  TCP超时重传定时器梳理 wireshark文档 offloading  ","permalink":"https://cloudnative.to/blog/kubernetes-1-17-vxlan-63s-delay/","tags":["tcpdump","vxlan","wireshark"],"title":"Kubernetes v1.17+ 集群下 CNI 使用 VXLAN 模式 SVC 有 63 秒延迟的触发原因定位"},{"categories":["OAM"],"contents":" (译)OAM和Crossplane: 构建现代应用的下一个阶段 OAM和Crossplane社区共同致力于建设一个聚焦在标准化的应用和基础设施上的开放社区。\n前言 在2020年三月份，在来自Crossplane社区的协作和巨大贡献下，开放应用模型（即OAM）项目发布了其v1alpha2规范，旨在为OAM本身和任何采用OAM的Kubernetes应用平台带来绝佳的可扩展性。在2020年5月份，随着Crossplane最新的v0.11版本发布，Crossplane现在具备了OAM规范的标准实现。我们十分激动看到两个社区间的合作，合作将标准的应用和基础设施定义与实施一起带入了云原生社区。\n旅程的开始 从Kubernetes工程师的角度来说，我们很接受现在的Kubernetes抽象层级：容器和基础设施API资源。但是对于平台的终端用户而言还是太过底层。\n为了在一定程度上提高终端用户的体验，一些团队试图通过引入PaaS或者GUI来向终端用户隐藏Kubernetes API。初看上去，这似乎是一个好主意。但事实上，这极大的限制了平台的能力。Kubernetes资源模型强调系统的所有能力都要能够可以表达成\u0026rdquo;数据\u0026rdquo;，例如API对象。向终端用户隐藏这些对象本质上会使得你的PaaS缺乏可扩展性，因而无法利用在生态圈中数不胜数的插件的能力。\n带着我们必须使平台构建者能够定义应用级别的抽象而不引入对平台可扩展性限制的理念，我们开始探索这个领域。\n建模你的应用，而不仅仅是描述 因为我们要定义应用级别的抽象，那么第一个问题就是：什么是应用？ 一个现代应用通常是若干部分的组合(如上图所示)。这样的模式广泛存在于现实世界：多层应用，机器学习训练应用（参数服务器和工作节点），更不用提微服务架构。但是经常被遗忘的是，这些应用的组件经常需要绑定一系列的运行策略。另外，分组策略也是一个特殊类型的运行策略。例如，我们需要在一个组内设置多个组件的安全组。\n因此直观的方法是使用CRD作为描述应用的高级抽象。并且这样可以与应用运行所需的所有其他部分（如运行策略、基础设施）一起合并成一个YAML文件，如下：\n上面的这个例子其实就是阿里巴巴“应用”定义的1.0版本。可以想象，开发人员会抱怨这样的“应用”太过于复杂，尽管它的初衷是使他们的生活更加简单。同样的，我们发现维护这个对象十分的混乱，并且基本上不可能扩展。更糟糕的是，越来越多的能力被安装到我们的Kubernetes集群中，这些都需要加进这个对象——Kubernetes社区发展的十分迅速！\n事实上，如果你仔细检查上述YAML文件，会发现开发者真正关心的只是运行他们应用的定义里的一些较小片段，如\u0026rdquo;commands\u0026rdquo;和\u0026rdquo;package\u0026rdquo;。\n因此为何我们不把这个YAML分解成多个片段呢？开发人员只需要根据他们自己掌握的部分定义\u0026rdquo;运行什么(what to run)\u0026ldquo;，运维人员（或者系统管理员）定义运行策略，基础设施运维人员处理基础设施部分。\n在接触了社区中的各个公司之后，我们发现“关注点分离”的想法与微软的团队非常契合。在与微软经过了数周的合作之后，我们定义了如下的顶层草图：\n看到了吗？与all-in-one式的CRD把所有东西揉在一起不同的是，OAM的核心思想本质上是一个\u0026rdquo;框架（frame）\u0026rdquo;。因此，开发人员和运维人员可以在整个应用表单的“空格”里填充他们自己片段的数据。这种灵活性保证了任何平台都可以采用这个定义而不会受限于特定的工作负载和能力类型，并且这个系统可以支持任何工作负载（容器、函数、甚至虚拟机）与运行能力（例如autoscaling、ingress、security policy)。\n我们称这种方法为“应用模型”，因为当一个用户需要组合多个片段为一个应用时需要遵循这个规范，他们需要去思考哪些空白需要去填充，例如是否是描述“运行什么”？或者是否是运行策略？这个过程和数学建模十分类似，数学建模使用数学概念和语言来描述系统。我们现在使用OAM概念来描述应用的不同部分。好处是现在平台可以理解这些不同片段的类别，这样可以保证片段的拓扑，或是检查运行策略的兼容性——可发现性和可管理性是现代产品级应用平台的核心。\n我们最终将这个理念发布为OAM spec v1alpha1\nCrossplane + OAM：构建Kubernetes之上的现代应用 OAM spec v1alpha1在阿里云的企业级分布式应用服务（EDAS）以及内部平台上得到了快速采用。然而，我们同样发现了一个在\u0026rdquo;运行什么\u0026rdquo;片段中的问题(之前称之为ComponentSchematic)，我们需要发布新版本的ComponentSchematic来进行YAML中的任何修改。这是因为它被设计成了一个模式（schematic）对象，因此开发者可以定义他们需要部署的任何工作负载并与他人分享。一个类似的问题同样存在于运行策略部分（我们称之为\u0026rdquo;traits\u0026rdquo;）——它的模式同样将schematic暴露给了终端用户。\n在12月份举行的KubeCon北美大会上，我们会见了来自Upbound.io的Crossplane维护者。我们讨论了OAM，以及如何通过利用CRD作为模式(CRD as schemas)的方法将OAM规范与Crossplane无缝集成。我们都认为这个方向是有希望的，在经过了数月的头脑风暴，提案以及无数次的激烈讨论之后，这个想法最终演进成为了如下的OAM spec v1alpha2:\nOAM spec v1alpha2采用了Kubernetes资源模型，因此Kubernetes中的任何数据片段都可以通过简单的定义一个WorkloadDefinition或者TraitDefinition来无缝引用为一个OAM中的工作负载或者特征(trait)。一个关于OAM spec v1alpha2的更深入的博客即将发布，这里可以先看看一个详细的说明。\n在实现方面，我们开发了一个基于Go的实现版本，称之为oam-kubernetes-runtime，作为Crossplane的一部分。现在我们有一个用于OAM的标准Kubernetes运行时。\n组合：完成整个图景 就像你可能看到的，我们仍然缺乏关于OAM的一个部分：我们如何定义组件依赖的基础设施片段，例如，一个来自阿里云MySQL数据库实例（RDS）？如何使这个定义适用于不同的云，就像OAM组件那样。\n在Kubernetes中定义这样应用中心的和可移植的基础设施绝非易事，社区中有一些operator和产品来做这个事情，但是没有像Crossplane中的Composition那样好。Composition组合多个基础设施片段，然后将其发布到与平台无关的CRD中，例如组合CRD来将VPC与RDS描述为一个新的数据库CRD。这个CRD，可以在之后引用为一个OAM的WorkloadDefinition并且成为一个应用的一部分。搞定！\n组合的结果十分的有力，以团队为中心的平台，可以让基础设施运维人员为应用定义和组合供应商无关的基础设施，并且可以使应用开发人员和应用运维人员以OAM的方式定义，运行和管理可移植的应用，不用再关心基础设施的复杂性。基础设施运维人员现在可以管理运行这些应用的基础设施。OAM和Crossplane一起提供了面向应用开发者和基础设施运维人员的优雅的解决方案。\n下一步？ OAM的核心理念是让开发人员描述自己的应用，使应用可以运行在一个无服务器平台，或者在一个本地的Kubernetes集群而无需修改应用的描述。这是阿里巴巴和微软一直在努力的云边协同（cloud/edge consistency）故事的一部分。很明显，与Crossplane的合作弥补了这个故事真正实现所缺失的重要部分，那就是在一个系统中同时涵盖统一的应用定义和基础设施定义。我们将继续努力使Crossplane成为OAM的标准Kubernetes实现，并且具有更好的工作负载/特征可移植性，互操作性，丰富的运行能力；构建一个聚焦于标准应用和基础设施的开放社区。\n（原文地址：OAM and Crossplane: The Next Stage for Building Modern Application）\n","permalink":"https://cloudnative.to/blog/oam-crossplane/","tags":["OAM","Microservices","Crossplane"],"title":"(译)OAM和Crossplane: 构建现代应用的下一个阶段"},{"categories":["Istio"],"contents":" 在上一篇文章一文带你彻底厘清 Kubernetes 中的证书工作机制中，我们介绍了 Kubernetes 中证书的工作机制。在这篇文章中，我们继续探讨 Istio 是如何使用证书来实现网格中服务的身份认证和安全通信的。\n本文是对 Istio 认证工作机制的深度分析，假设读者已经了解 Service Mesh 以及 Istio 的相关基础概念，因此在本文对此类基础概念不再解释。对于 Istio 不熟悉的读者，建议先阅读 Istio 官方网站上的的这篇基础介绍 What is Istio?。\nIstio 安全架构 Istio 为微服务提供了无侵入，可插拔的安全框架。应用不需要修改代码，就可以利用 Istio 提供的双向 TLS 认证实现服务身份认证，并基于服务身份信息提供细粒度的访问控制。Istio 安全的高层架构如下图所示：\n图1. Istio Security Architecture，图片来源istio.io\n图中展示了 Istio 中的服务认证和授权两部分内容。让我们暂时忽略掉授权部分，先关注认证部分。服务认证是通过控制面和数据面一起实现的： * 控制面：Istiod 中实现了一个 CA （Certificate Authority，证书机构） 服务器。该 CA 服务器负责为网格中的各个服务签发证书，并将证书分发给数据面的各个服务的sidecar代理。 * 数据面：在网格中的服务相互之间发起 plain HTTP/TCP 通信时，和服务同一个 pod 中的sidecar代理会拦截服务请求，采用证书和对端服务的sidecar代理进行双向 TLS 认证并建立一个 TLS 连接，使用该 TLS 连接来在网络中传输数据。\n控制面证书签发流程 图1是对 Istio 安全架构的一个高度概括的描述，让我们把图1中控制面的交互展开，看一下其中的细节。\n图2. Istio 证书分发流程\n我们先暂时忽略图中右边蓝色虚线的部分（稍后会在 控制面身份认证 部分讲到），图中左半部分描述了 Istio 控制面向 Envoy 签发证书的流程： 1. Envoy 向 pilot-agent 发起一个 SDS (Secret Discovery Service) 请求，要求获取自己的证书和私钥。 2. Pilot-agent 生成私钥和 CSR （Certificates Signing Request，证书签名请求），向 Istiod 发送证书签发请求，请求中包含 CSR 和该 pod 中服务的身份信息。 3. Istiod 根据请求中服务的身份信息（Service Account）为其签发证书，将证书返回给 Pilot-agent。 4. Pilot-agent 将证书和私钥通过 SDS 接口返回给 Envoy。\n为什么要通过 Pilot-agent 中转？ 从图2可以看到，Istio 证书签发的过程中涉及到了三个组件： Istiod (Istio CA) \u0026mdash;\u0026gt; Pilot-agent \u0026mdash;\u0026gt; Enovy。为什么其他 xDS 接口都是由 Istiod 直接向 Envoy 提供，但 SDS 却要通过 Pilot-agent 进行一次中转，而不是直接由 Envoy 通过 SDS 接口从 Istiod 获取证书呢？这样做主要有两个原因。\n首先，在 Istio 的证书签发流程中，由 Pilot-agent 生成私钥和 CSR，再通过 CSR 向 Istiod 中的 CA 申请证书。在整个过程中，私钥只存在于本地的 Istio-proxy 容器中。如果去掉中间 Pilot-agent 这一步，直接由 Envoy 向 Isitod 申请证书，则需要由 Istiod 生成私钥，并将私钥和证书一起通过网络返回给 Envoy，这将大大增加私钥泄露的风险。\n另一方面，通过 Pilot-agent 来提供 SDS 服务，由 Pilot-agent 生成标准的 CSR 证书签名请求，可以很容易地对接不同的 CA 服务器，方便 Istio 和其他证书机构进行集成。\n控制面身份认证 要通过服务证书来实现网格中服务的身份认证，必须首先确保服务从控制面获取自身证书的流程是安全的。Istio 通过 Istiod 和 Pilog-agent 之间的 gRPC 通道传递 CSR 和证书，因此在这两个组件进行通信时，双方需要先验证对方的身份，以避免恶意第三方伪造 CSR 请求或者假冒 Istiod CA 服务器。在目前的版本中(Istio1.6)，Pilot-agent 和 Istiod 分布采用了不同的认证方式。\n Istiod 身份认证  Istiod 采用其内置的 CA 服务器为自身签发一个服务器证书（图2中的 Istiod certificate），并采用该服务器证书对外提供基于 TLS 的 gPRC 服务。 Istiod 调用 Kube-apiserver 生成一个 ConfigMap， 在该 ConfigMap 中放入了 Istiod 的 CA 根证书(图2中的 istio-ca-root-cert)。 该 ConfigMap 被 Mount 到 Istio-proxy 容器中，被 Pilot-agent 用于验证 Istiod 的服务器证书。 在 Pilot-agent 和 Istiod 建立 gRPC 连接时，Pilot-agent 采用标准的 TLS 服务器认证流程对 Istiod 的服务器证书进行认证。  Pilot-agent 身份认证  在 Kubernetes 中可以为每一个 pod 关联一个 Service Account，以表明该 pod 中运行的服务的身份信息。例如 bookinfo 中 reviews 服务的 service accout 是 “bookinfo-reviews” 。 Kubernetes 会为该 service account 生成一个 jwt token，并将该 token 通过 secret 加载到 pod 中的一个文件。 Pilot-agent 在向 Istiod 发送 CSR 时，将其所在 pod 的 service account token 也随请求发送给 Istiod。 Istiod 调用 Kube-apiserver 接口验证请求中附带的 service account token，以确认请求证书的服务身份是否合法。   备注：除了 Kubernetes 之外， Istio 也支持虚机部署，在虚机部署的场景下，由于没有 service account，Pilot-agent 和 Pilotd 之间的身份认证方式有所不同。由于 Istio 的主要使用场景还是 Kubernetes，本文只分析 Kubernetes 部署场景。\nSDS 工作原理 和其他 xDS 接口一样，SDS 也是 Envoy 支持的一种动态配置服务接口。Envoy 可以通过 SDS（secret discovery service） 接口从 SDS 服务器自动获取证书。和之前的方式相比，SDS 最大的好处就是简化了证书管理。在没有使用 SDS 前，Istio 中的服务证书被创建为 Kubernetes secret，并挂载到代理容器中。如果证书过期了，则需要更新 secret 并重启 Envoy 容器，以启用新的证书。使用SDS后，SDS 服务器（Pilot-agent充当了 SDS 服务器的角色）将向 Envoy 实例主动推送证书。如果证书过期，SDS 服务器只需将新的证书推送到 Envoy 例中，Envoy 会使用新的证书来创建链接，无需重新启动。\n图3. Envoy SDS 服务\n可以看到，Istio 采用 SDS 后，避免了在证书更新后重启 Envoy，大大减少了证书更新对业务的影响。同时，由于 Pilot-agent 和 Envoy 处于同一容器中，私钥只存在于本地容器，避免了在网络中传递私钥，也降低了私钥泄露的安全风险。\nSDS 服务向 Envoy 下发的数据结构为extensions.transport_sockets.tls.v3.Secret,其结构如下：\n{ \u0026quot;name\u0026quot;: \u0026quot;...\u0026quot;, // Secret 名称 \u0026quot;tls_certificate\u0026quot;: \u0026quot;{...}\u0026quot;, // 数字证书 \u0026quot;session_ticket_keys\u0026quot;: \u0026quot;{...}\u0026quot;, \u0026quot;validation_context\u0026quot;: \u0026quot;{...}\u0026quot;, // 证书验证信息 \u0026quot;generic_secret\u0026quot;: \u0026quot;{...}\u0026quot; }  其中在 Istio 中用到的主要是 tls_certificate 和 validation_context。 分别用于传递数字证书和验证对方证书使用到的根证书。下面是这两个字段的结构，结构中标注了我们主要需要关注的内容。\n```json { \u0026quot;certificate_chain\u0026quot;: \u0026quot;{...}\u0026quot;, // 证书内容 \u0026quot;private_key\u0026quot;: \u0026quot;{...}\u0026quot;, // 证书的私钥 \u0026quot;private_key_provider\u0026quot;: \u0026quot;{...}\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;{...}\u0026quot; }  ```json { \u0026quot;trusted_ca\u0026quot;: \u0026quot;{...}\u0026quot;, // CA 根证书 \u0026quot;verify_certificate_spki\u0026quot;: [], \u0026quot;verify_certificate_hash\u0026quot;: [], \u0026quot;match_subject_alt_names\u0026quot;: [], // 需要验证的 subject alt name \u0026quot;crl\u0026quot;: \u0026quot;{...}\u0026quot;, \u0026quot;allow_expired_certificate\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;trust_chain_verification\u0026quot;: \u0026quot;...\u0026quot; }  网格 Sidecar 证书配置 在 Istio 的 Enovy sidecar 配置中，有两处需要通过 SDS 来配置证书：\n Inbound Listener：由于Enovy 通过 Listener 对外提供服务，需要通过 SDS 配置服务器证书，服务器证书私钥，以及验证下游客户端证书的 CA 根证书。 Outbound Cluster：对于上游的 Cluster 而言，Envoy 是客户端的角色，因此需要在 Cluster 中通过 SDS 配置客户端证书，客户端证书私钥，以及验证上游服务器的 CA 根证书。  下面我们来看一下 bookinfo 示例中 Envoy sidecar代理上 reviews 微服务相关的证书配置，以对 Istio 中 SDS 的运作机制有一个更清晰的认识。为了简略起见，本文只显示了部分关键的配置。你也可以查看 Github 上的完整配置。\n通过 Envoy 的管理端口，可以导出 Envoy 中的当前配置，导出命令如下：\nkubectl exec reviews-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/config_dump \u0026gt; config_dump  配置文件中 SDS 服务器的定义如下，Pilot-agent 在 /etc/istio/proxy/SDS 这路径上通过 unix domain socket 提供了一个 SDS 服务器。\n{ \u0026quot;name\u0026quot;: \u0026quot;sds-grpc\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;STATIC\u0026quot;, \u0026quot;connect_timeout\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;hidden_envoy_deprecated_hosts\u0026quot;: [ { \u0026quot;pipe\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/etc/istio/proxy/SDS\u0026quot; } } ], \u0026quot;http2_protocol_options\u0026quot;: {} }  Details Pod 在 9080 端口对外提供服务，因此需要通过 SDS 配置 9080 端口上的服务端证书和验证客户端证书的 CA 根证书。\n{ \u0026quot;name\u0026quot;: \u0026quot;virtualInbound\u0026quot;, // 15006端口上的虚拟入向监听器 \u0026quot;active_state\u0026quot;: { \u0026quot;version_info\u0026quot;: \u0026quot;2020-05-14T03:59:54Z/25\u0026quot;, \u0026quot;listener\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.Listener\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;virtualInbound\u0026quot;, \u0026quot;address\u0026quot;: { \u0026quot;socket_address\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;port_value\u0026quot;: 15006 } }, \u0026quot;filter_chains\u0026quot;: [ { \u0026quot;filter_chain_match\u0026quot;: { \u0026quot;prefix_ranges\u0026quot;: [ { \u0026quot;address_prefix\u0026quot;: \u0026quot;10.44.0.8\u0026quot;, \u0026quot;prefix_len\u0026quot;: 32 } ], \u0026quot;destination_port\u0026quot;: 9080 // 用于处理发向reviews服务9080端口的业务请求的filter chain }, \u0026quot;filters\u0026quot;: [...], \u0026quot;transport_socket\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;envoy.transport_sockets.tls\u0026quot;, \u0026quot;typed_config\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\u0026quot;, \u0026quot;common_tls_context\u0026quot;: { \u0026quot;alpn_protocols\u0026quot;: [ \u0026quot;h2\u0026quot;, \u0026quot;http/1.1\u0026quot; ], \u0026quot;tls_certificate_sds_secret_configs\u0026quot;: [ // 配置服务器端证书 { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, // 服务器证书 Secret 名称 \u0026quot;sds_config\u0026quot;: { \u0026quot;api_config_source\u0026quot;: { \u0026quot;api_type\u0026quot;: \u0026quot;GRPC\u0026quot;, \u0026quot;grpc_services\u0026quot;: [ { \u0026quot;envoy_grpc\u0026quot;: { \u0026quot;cluster_name\u0026quot;: \u0026quot;sds-grpc\u0026quot; // 配置用于获取服务器端证书的 SDS 服务器 } } ] } } } ], \u0026quot;combined_validation_context\u0026quot;: { \u0026quot;default_validation_context\u0026quot;: {}, \u0026quot;validation_context_sds_secret_config\u0026quot;: { // 配置验证客户端证书的 CA 根证书 \u0026quot;name\u0026quot;: \u0026quot;ROOTCA\u0026quot;, // CA 根证书 Secret 名称 \u0026quot;sds_config\u0026quot;: { \u0026quot;api_config_source\u0026quot;: { \u0026quot;api_type\u0026quot;: \u0026quot;GRPC\u0026quot;, \u0026quot;grpc_services\u0026quot;: [ { \u0026quot;envoy_grpc\u0026quot;: { \u0026quot;cluster_name\u0026quot;: \u0026quot;sds-grpc\u0026quot; // 配置用于获取 CA 根证书的 SDS 服务器 } } ] } } } } }, \u0026quot;require_client_certificate\u0026quot;: true } } } ], } }  客户端 Pod 上的 Enovy 通过 reviews outbound cluster 访问上游 reviews 服务，因此需要在该 cluster 上配置客户端证书以及验证服务器端证书的 CA 根证书。在这里我们需要注意的是，Envoy 在验证服务器端证书时会同时验证证书中的 subject alternative name 字段。该字段中设置的是 reviews 服务 Pod 关联的 Service Account 名称。 由于 Service Account 是 Istio 中认可的一种用户账户，因此通过为 Service Account 设置不同的资源访问权限，可以进一步实现细粒度的权限控制，例如按照 URL 进行授权。\n{ \u0026quot;version_info\u0026quot;: \u0026quot;2020-05-14T03:15:47Z/18\u0026quot;, \u0026quot;cluster\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.Cluster\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;outbound|9080||reviews.default.svc.cluster.local\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;EDS\u0026quot;, \u0026quot;eds_cluster_config\u0026quot;: { \u0026quot;eds_config\u0026quot;: { \u0026quot;ads\u0026quot;: {} }, \u0026quot;service_name\u0026quot;: \u0026quot;outbound|9080||reviews.default.svc.cluster.local\u0026quot; }, \u0026quot;service_name\u0026quot;: \u0026quot;outbound|9080||reviews.default.svc.cluster.local\u0026quot; }, \u0026quot;circuit_breakers\u0026quot;: {...}, \u0026quot;filters\u0026quot;: [...], \u0026quot;transport_socket_matches\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;tlsMode-istio\u0026quot;, \u0026quot;match\u0026quot;: { \u0026quot;tlsMode\u0026quot;: \u0026quot;istio\u0026quot; }, \u0026quot;transport_socket\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;envoy.transport_sockets.tls\u0026quot;, \u0026quot;typed_config\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\u0026quot;, \u0026quot;common_tls_context\u0026quot;: { \u0026quot;alpn_protocols\u0026quot;: [ \u0026quot;istio-peer-exchange\u0026quot;, \u0026quot;istio\u0026quot; ], \u0026quot;tls_certificate_sds_secret_configs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, // 配置用于访问 reviews 服务的客户端证书 \u0026quot;sds_config\u0026quot;: { \u0026quot;api_config_source\u0026quot;: { \u0026quot;api_type\u0026quot;: \u0026quot;GRPC\u0026quot;, \u0026quot;grpc_services\u0026quot;: [ { \u0026quot;envoy_grpc\u0026quot;: { \u0026quot;cluster_name\u0026quot;: \u0026quot;sds-grpc\u0026quot; // 配置用于获取客户端证书的 SDS 服务器 } } ] } } } ], \u0026quot;combined_validation_context\u0026quot;: { \u0026quot;default_validation_context\u0026quot;: { \u0026quot;verify_subject_alt_name\u0026quot;: [ \u0026quot;spiffe://cluster.local/ns/default/sa/bookinfo-reviews\u0026quot; // 验证服务器证书时需要验证 SAN 中的 service account 名称 ] }, \u0026quot;validation_context_sds_secret_config\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;ROOTCA\u0026quot;, // 配置验证 reviews 服务器证书的 CA 根证书 \u0026quot;sds_config\u0026quot;: { \u0026quot;api_config_source\u0026quot;: { \u0026quot;api_type\u0026quot;: \u0026quot;GRPC\u0026quot;, \u0026quot;grpc_services\u0026quot;: [ { \u0026quot;envoy_grpc\u0026quot;: { \u0026quot;cluster_name\u0026quot;: \u0026quot;sds-grpc\u0026quot; // 配置用于获取 CA 根证书的 SDS 服务器 } } ] } } } } }, \u0026quot;sni\u0026quot;: \u0026quot;outbound_.9080_._.reviews.default.svc.cluster.local\u0026quot; } } }, ] }, \u0026quot;last_updated\u0026quot;: \u0026quot;2020-05-14T03:16:33.061Z\u0026quot; }  上面配置中 SAN 中的 service account 名称来自于 reviews pod 中的 service account 配置。\napiVersion: apps/v1 kind: Deployment metadata: name: reviews-v1 labels: app: reviews version: v1 spec: replicas: 1 selector: matchLabels: app: reviews version: v1 template: metadata: labels: app: reviews version: v1 spec: serviceAccountName: bookinfo-reviews // 设置 reviews pod 的 service account containers: - name: reviews image: docker.io/istio/examples-bookinfo-reviews-v1:1.15.0 imagePullPolicy: IfNotPresent env: - name: LOG_DIR value: \u0026quot;/tmp/logs\u0026quot; ports: - containerPort: 9080 ...  在导出的配置中可以看到 Envoy 通过 SDS 服务器获取到的证书（为了简略起见，省略了证书中间的部分内容）。\n{ \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.admin.v3.SecretsConfigDump\u0026quot;, \u0026quot;dynamic_active_secrets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;version_info\u0026quot;: \u0026quot;05-14 03:16:30.900\u0026quot;, \u0026quot;last_updated\u0026quot;: \u0026quot;2020-05-14T03:16:31.125Z\u0026quot;, \u0026quot;secret\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;tls_certificate\u0026quot;: { \u0026quot;certificate_chain\u0026quot;: { \u0026quot;inline_bytes\u0026quot;: \u0026quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FUXXXXXXXXXXXXXXUZJQ0FURS0tLS0tCg==\u0026quot; // details 服务的证书，该证书被同时用作了服务器证书和客户端证书 }, \u0026quot;private_key\u0026quot;: { \u0026quot;inline_bytes\u0026quot;: \u0026quot;W3JlZGFjdGVkXQ==\u0026quot; // detatils 服务证书对应的私钥 } } } }, { \u0026quot;name\u0026quot;: \u0026quot;ROOTCA\u0026quot;, \u0026quot;version_info\u0026quot;: \u0026quot;2020-05-14 03:16:31.300416193 +0000 UTC m=+1.537483882\u0026quot;, \u0026quot;last_updated\u0026quot;: \u0026quot;2020-05-14T03:16:31.343Z\u0026quot;, \u0026quot;secret\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ROOTCA\u0026quot;, \u0026quot;validation_context\u0026quot;: { \u0026quot;trusted_ca\u0026quot;: { \u0026quot;inline_bytes\u0026quot;: \u0026quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0XXXXXXXXXXXXXXXXXXXXJQ0FURS0tLS0tCg==\u0026quot; // 用于验证通信对方证书的 CA 根证书 } } } } ] }  通过上面的配置，我们可以看到，虽然需要在 Enovy sidecar配置文件中不同的位置为 Envoy 配置服务器、客户端证书以及验证对方的 CA 根证书，但 Istio 中实际上只采用了一个服务证书和 CA 根证书。Isito 将名称为 default 的证书被同时用于 Inbound Listener 的服务器证书和 Outbound Cluster 的客户端证书，并将名称为 ROOTCA 的证书被用于验证下游客户端证书和上游服务器证书的根证书。\nGateway 证书配置 除了需要和网格内部的服务进行通信之外，Ingress Gateway 和 Egress Gateway 还需要连接到网格外部的系统。如果这些外部连接也需要采用 TLS，则 Gateway 中也要配置这些外部系统的相关证书。\nIngress Gateway 中需要如下证书相关的配置： * 作为客户端和网格内部其他服务进行通信的客户端证书和私钥，和其他服务使用的证书类似，该证书也是由 Istio CA 颁发的。 * 验证网格内其他服务证书的 CA 根证书，该根证书是 Istio CA 的根证书。 * 作为网关向网格外部提供服务使用的服务器端证书和私钥，该证书一般是由一个权威 CA 或者第三方 CA 签发的。如果有多个 host，需要为每一个 host 分别配置配置不同的证书。 * 如果对外提供的服务需要双向 TLS 认证，还需要配置用于验证客户端证书的 CA 根证书。\nEgress Gateway 中需要如下证书相关的配置： * 作为服务器接受网格内部其他服务访问的服务器证书和私钥，和其他服务使用的证书类似，该证书也是由 Istio CA 颁发的。 * 验证网格内其他服务证书的 CA 根证书，该根证书是 Istio CA 的根证书。 * 作为出口网关访问外部服务时，如果该外部服务采用了 TLS，则需要配置一个验证该服务器证书的 CA 根证书来验证该服务器。该根证书一般是一个权威 CA 或者第三方 CA。 * 如果访问的外部服务要求双向 TLS 认证，则还需要网关配置一个该外部服务认可的客户端证书。该证书一般是由一个权威 CA 或者第三方 CA 签发的。\n由于 Gateway 中配置的和外部系统相关的证书不是通过 SDS 从 Istio CA 获取的，而是采用第三方 CA 颁发的，因此到期后并不能自动更新，而需要手动进行更新。因此需要注意这些证书的有效期，在证书过期前及时重新申请证书并更新到 Gateway 配置中，以避免影响业务。\n在 Gateway 上配置第三方证书的方法是采用 Kubernetes Secret 和 Istio Gateway CRD。例如我们可以采用下面的步骤在 Ingress Gateway 上配置对外提供服务使用的服务器证书。\n首先创建一个 secret，该 secret 中包含了服务器证书和私钥：\nkubectl create -n istio-system secret tls bookinfo-credential --key=bookinfo.example.com.key --cert=bookinfo.example.com.crt  然后通过 Gateway CRD 定义一个对外提供服务的虚拟主机，并指定使用刚才定义的 secret。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: mygateway spec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: httpbin-credential # 在此处设置包含了服务器证书和私钥的 secret hosts: - bookinfo.example.com  Istio 将此配置通过 xDS 接口下发到 Ingress Gateway Pod 中的 Envoy 上，可以在该 Envoy 的配置导出中看到 Ingress 网关对外提供的 443 端口上的证书配置（配置文件中的端口是8443，这是因为 Pod 内使用了8443端口，但对外暴露的 LoadBalancer 上的端口是443）。\n{ \u0026quot;name\u0026quot;: \u0026quot;0.0.0.0_8443\u0026quot;, \u0026quot;active_state\u0026quot;: { \u0026quot;version_info\u0026quot;: \u0026quot;2020-05-27T07:43:51Z/21\u0026quot;, \u0026quot;listener\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.Listener\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;0.0.0.0_8443\u0026quot;, \u0026quot;address\u0026quot;: { \u0026quot;socket_address\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;port_value\u0026quot;: 8443 } }, \u0026quot;filter_chains\u0026quot;: [ { \u0026quot;filter_chain_match\u0026quot;: { \u0026quot;server_names\u0026quot;: [ \u0026quot;bookinfo.example.com\u0026quot; ] }, \u0026quot;filters\u0026quot;: [...], \u0026quot;transport_socket\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;envoy.transport_sockets.tls\u0026quot;, \u0026quot;typed_config\u0026quot;: { \u0026quot;@type\u0026quot;: \u0026quot;type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\u0026quot;, \u0026quot;common_tls_context\u0026quot;: { \u0026quot;alpn_protocols\u0026quot;: [ \u0026quot;h2\u0026quot;, \u0026quot;http/1.1\u0026quot; ], \u0026quot;tls_certificate_sds_secret_configs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;bookinfo-credential\u0026quot;, // Ingress Gateway 中配置的 Kubernetes secret \u0026quot;sds_config\u0026quot;: { \u0026quot;api_config_source\u0026quot;: { \u0026quot;api_type\u0026quot;: \u0026quot;GRPC\u0026quot;, \u0026quot;grpc_services\u0026quot;: [ { \u0026quot;google_grpc\u0026quot;: { \u0026quot;target_uri\u0026quot;: \u0026quot;unix:/var/run/ingress_gateway/sds\u0026quot;, // 从本地 unix domain socket 上的 SDS 服务器获取服务器证书 \u0026quot;stat_prefix\u0026quot;: \u0026quot;sdsstat\u0026quot; } } ] } } } ] }, \u0026quot;require_client_certificate\u0026quot;: false } } } ], }, } }  从配置中可以看出，Ingress Gateway 使用的服务器证书也是通过 SDS 服务获取的。Pilot-agent 在路径unix:/var/run/ingress_gateway/sds 上为 Ingress Gateway 提供了一个基于 unix domain socket 的 SDS 服务。Ingress Gateway 中的 Envoy 向该 SDS 服务器请求上述配置文件中的 secret，Pilot-agent 从 Kubernetes 中查到该 同名 secret，然后转换为 SDS 消息返回给 Envoy。\n备注： 1. Ingress Gateway 用于和网格内其他服务通信的服务身份证书还是由 Istio CA 颁发的，其证书获取的流程同图2。 2. Egress Gateway 未使用 SDS 获取用于访问外部服务的客户端证书（1.6 现状，后续也许会修改）。\n图4. Ingress Gateway 证书获取流程\n数据面使用的所有证书 下图中以 bookinfo 来举例说明 Istio 在数据面使用到的所有证书。为了方便说明 Gateway 的证书配置，我们假设在 Ingress Gateway 上以 bookinfo.example.com 的主机名对外提供服务，并且 ratings 服务通过 Egress Gateway 访问了一个网格外部的第三方 TLS 服务。\n图中不同颜色边框的图标代表了不同的证书。该示例中一共使用了七个不同的证书，分别为3个服务的证书（同时用作服务器和客户端证书），Ingress Gateway 自身的客户端证书，Ingress Gateway 对外部提供服务的服务器证书，Egress Gateway 自身的服务器证书，Egress Gateway 访问外部服务使用的客户端证书。\n除了 Ingress Gateway 对外提供服务的服务器证书和 Egress Gateway 访问第三方服务的客户端证书之外，其他证书都是 Envoy 通过 SDS 服务从 Istio CA 获取的，因此都使用 Istio Root CA 证书进行验证。这两个第三方证书则需要采用第三方 CA 根证书进行验证。\n图5. Istio 数据面使用到的所有证书\n小结 微服务应用本质上是一个分布式的网络程序，在微服务应用内存在大量的服务间网络通信。在云化部署环境中，服务间的身份认证和安全通信是微服务面临的一大挑战。Istio 建立了一套以数字证书为基础的服务认证安全框架，在不修改应用的前提下提供了服务之间的身份认证和安全通信，并以身份认证为基础提供了强大的授权机制。\n参考文档  Istio Secure Gateways Istio Egress Gateways with TLS Origination  ","permalink":"https://cloudnative.to/blog/istio-certificate/","tags":["Istio"],"title":"一文带你彻底厘清 Isito 中的证书工作机制"},{"categories":["DevOps"],"contents":" 作者：张晓辉：资深码农，12 年软件开发经验。曾在汇丰软件、唯品会、数人云等公司任职。目前就职小鹏汽车，在基础架构团队从事技术中台的研发。\n这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。\n快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。\nPipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task  Tekton Pipelines提供了上面的CRD，其中部分CRD与Kubernetes core中资源相对应\n Task =\u0026gt; Pod Task.Step =\u0026gt; Container  工作原理 (图片来自tekton.dev)\nTekton Pipeline 是基于 Knative 的实现，pod tekton-pipelines-controller 中有两个 Knative Controller的实现：PipelineRun 和 TaskRun。\nTask的执行顺序 PipelineRun Controller 的 #reconcile()方法，监控到有PipelineRun被创建。然后从PipelineSpec的 tasks 列表，构建出一个图（graph），用于描述Pipeline中 Task 间的依赖关系。依赖关系是通过runAfter和from，进而控制Task的执行顺序。与此同时，准备PipelineRun中定义的PipelineResources。\n// Node represents a Task in a pipeline. type Node struct { // Task represent the PipelineTask in Pipeline Task Task // Prev represent all the Previous task Nodes for the current Task Prev []*Node // Next represent all the Next task Nodes for the current Task Next []*Node } // Graph represents the Pipeline Graph type Graph struct { //Nodes represent map of PipelineTask name to Node in Pipeline Graph Nodes map[string]*Node } func Build(tasks Tasks) (*Graph, error) { ... }  PipelineRun中定义的参数（parameters）也会注入到PipelineSpec中：\npipelineSpec = resources.ApplyParameters(pipelineSpec, pr)  接下来就是调用dag#GetSchedulable()方法，获取未完成（通过Task状态判断）的 Task 列表；\nfunc GetSchedulable(g *Graph, doneTasks ...string) (map[string]struct{}, error) { ... }  为 Task A 创建TaskRun，假如Task配置了Condition。会先为 condition创建一个TaskRun，只有在 condition 的TaskRun运行成功，才会运行 A 的TaskRun；否则就跳过。\nStep的执行顺序 这一部分篇幅较长，之前的文章 控制 Pod 内容器的启动顺序 中提到过。\n这里补充一下Kubernetes Downward API的使用，Kubernetes Downward API的引入，控制着 Task 的第一个 Step 在何时执行。\nTaskRun Controller 在 reconciling 的过程中，在相应的 Pod 状态变为Running时，会将tekton.dev/ready=READY写入到 Pod 的 annotation 中，来通知第一个Step的执行。\nPod的部分内容：\nspec: containers: - args: - -wait_file - /tekton/downward/ready - -wait_file_content - -post_file - /tekton/tools/0 - -termination_path - /tekton/termination - -entrypoint - /ko-app/git-init - -- - -url - ssh://git@gitlab.nip.io:8022/addozhang/logan-pulse.git - -revision - develop - -path - /workspace/git-source command: - /tekton/tools/entrypoint volumeMounts: - mountPath: /tekton/downward name: tekton-internal-downward volumes: - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.annotations['tekton.dev/ready'] path: ready name: tekton-internal-downward  对原生的排序step container进一步处理：启动命令使用entrypoint提供，并设置执行参数：\nentrypoint.go\nfunc orderContainers(entrypointImage string, steps []corev1.Container, results []v1alpha1.TaskResult) (corev1.Container, []corev1.Container, error) { initContainer := corev1.Container{ Name: \u0026quot;place-tools\u0026quot;, Image: entrypointImage, Command: []string{\u0026quot;cp\u0026quot;, \u0026quot;/ko-app/entrypoint\u0026quot;, entrypointBinary}, VolumeMounts: []corev1.VolumeMount{toolsMount}, } if len(steps) == 0 { return corev1.Container{}, nil, errors.New(\u0026quot;No steps specified\u0026quot;) } for i, s := range steps { var argsForEntrypoint []string switch i { case 0: argsForEntrypoint = []string{ // First step waits for the Downward volume file. \u0026quot;-wait_file\u0026quot;, filepath.Join(downwardMountPoint, downwardMountReadyFile), \u0026quot;-wait_file_content\u0026quot;, // Wait for file contents, not just an empty file. // Start next step. \u0026quot;-post_file\u0026quot;, filepath.Join(mountPoint, fmt.Sprintf(\u0026quot;%d\u0026quot;, i)), \u0026quot;-termination_path\u0026quot;, terminationPath, } default: // All other steps wait for previous file, write next file. argsForEntrypoint = []string{ \u0026quot;-wait_file\u0026quot;, filepath.Join(mountPoint, fmt.Sprintf(\u0026quot;%d\u0026quot;, i-1)), \u0026quot;-post_file\u0026quot;, filepath.Join(mountPoint, fmt.Sprintf(\u0026quot;%d\u0026quot;, i)), \u0026quot;-termination_path\u0026quot;, terminationPath, } } ... }  自动运行的容器 这些自动运行的容器作为 pod 的initContainer会在 step 容器运行之前运行\ncredential-initializer 用于将 ServiceAccount 的相关secrets持久化到容器的文件系统中。比如 ssh 相关秘钥、config文件以及know_hosts文件；docker registry 相关的凭证则会被写入到 docker 的配置文件中。\nworking-dir-initializer 收集Task内的各个Step的workingDir配置，初始化目录结构\nplace-scripts 假如Step使用的是script配置（与command+args相对），这个容器会将脚本代码（script字段的内容）持久化到/tekton/scripts目录中。\n注：所有的脚本会自动加上#!/bin/sh\\nset -xe\\n，所以script字段里就不必写了。\nplace-tools 将entrypoint的二进制文件，复制到/tekton/tools/entrypoint.\nTask/Step间的数据传递 针对不同的数据，有多种不同的选择。比如Workspace、Result、PipelineResource。对于由于Task的执行是通过Pod来完成的，而Pod会调度到不同的节点上。因此Task间的数据传递，需要用到持久化的卷。\n而Step作为Pod中的容器来运行，\nWorkspace 工作区，可以理解为一个挂在到容器上的卷，用于文件的传递。\npersistentVolumeClaim 引用已存在persistentVolumeClaim卷（volume）。这种工作空间，可多次使用，需要先进行创建。比如 Java 项目的 maven，编译需要本地依赖库，这样可以节省每次编译都要下载依赖包的成本。\nworkspaces: - name: m2 persistentVolumeClaim: claimName: m2-pv-claim  apiVersion: v1 kind: PersistentVolume metadata: name: m2-pv labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteMany hostPath: path: \u0026quot;/data/.m2\u0026quot; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: m2-pv-claim spec: storageClassName: manual # volumeName: m2-pv accessModes: - ReadWriteMany resources: requests: storage: 10Gi  volumeClaimTemplate 为每个PipelineRun或者TaskRun创建PersistentVolumeClaim卷（volume）的模板。比如一次构建需要从 git 仓库克隆代码，而针对不同的流水线代码仓库是不同的。这里就会用到volumeClaimTemplate，为每次构建创建一个PersistentVolumeClaim卷。（从0.12.0开始）\n生命周期同PipelineRun或者TaskRun，运行之后释放。\nworkspaces: - name: git-source volumeClaimTemplate: spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi  相较于persistantVolumeClain类型的workspace，volumeClaimTemplate不需要在每次在PipelineRun完成后清理工作区；并发情况下可能会出现问题。\nemptyDir 引用emptyDir卷，跟随Task生命周期的临时目录。适合在Task的Step间共享数据，无法在多个Task间共享。\nworkspaces: - name: temp emptyDir: {}  configMap 引用一个configMap卷，将configMap卷作为工作区，有如下限制：\n 挂载的卷是只读的 需要提前创建configMap configMap的大小限制为1MB（Kubernetes的限制）  使用场景，比如使用maven编译Java项目，配置文件settings.xml可以使用configMap作为工作区\nworkspaces: - name: maven-settings configmap: name: maven-settings  secret 用于引用secret卷，同configMap工作区一样，也有限制：\n 挂载的卷是只读的 需要提前创建secret secret的大小限制为1MB（Kubernetes的限制）  results results字段可以用来配置多个文件用来存储Tasks的执行结果，这些文件保存在/tekton/results目录中。\n在Pipeline中，可以通过tasks.[task-nanme].results.[result-name]注入到其他Task的参数中。\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: print-date annotations: description: | A simple task that prints the date spec: results: - name: current-date-unix-timestamp description: The current date in unix timestamp format - name: current-date-human-readable description: The current date in human readable format steps: - name: print-date-unix-timestamp image: bash:latest script: | #!/usr/bin/env bash date +%s | tee $(results.current-date-unix-timestamp.path) - name: print-date-humman-readable image: bash:latest script: | #!/usr/bin/env bash date | tee $(results.current-date-human-readable.path) --- apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: pass-date spec: pipelineSpec: tasks: - name: print-date taskRef: name: print-date - name: read-date runAfter: #配置执行顺序 - print-date taskSpec: params: - name: current-date-unix-timestamp type: string - name: current-date-human-readable type: string steps: - name: read image: busybox script: | echo $(params.current-date-unix-timestamp) echo $(params.current-date-human-readable) params: - name: current-date-unix-timestamp value: $(tasks.print-date.results.current-date-unix-timestamp) # 注入参数 - name: current-date-human-readable value: $(tasks.print-date.results.current-date-human-readable) # 注入参数  执行结果：\n┌──────Logs(tekton-pipelines/pass-date-read-date-rhlf2-pod-9b2sk)[all] ────────── │ │ place-scripts stream closed ││ step-read 1590242170 │ │ step-read Sat May 23 13:56:10 UTC 2020 ││ step-read + echo 1590242170 │ │ step-read + echo Sat May 23 13:56:10 UTC 2020 │ │ place-tools stream closed │ │ step-read stream closed │ │  PipelineResource PipelineResource在最后提，因为目前只是alpha版本，何时会进入beta或者弃用目前还是未知数。有兴趣的可以看下这里：Why Aren’t PipelineResources in Beta?\n简单来说，PipelineResource可以通过其他的方式实现，而其本身也存在弊端：比如实现不透明，debug有难度；功能不够强；降低了Task的重用性等。\n比如git类型的PipelineResource，可以通过workspace和git-clone Task来实现；存储类型的，也可以通过workspace来实现。\n这也就是为什么上面介绍workspace的篇幅比较大。个人也偏向于使用workspace，灵活度高；使用workspace的Task重用性强。\n参考  云原生 CICD: Tekton Pipeline 实战 控制 Pod 内容器的启动顺序 Knative Controller Why Aren’t PipelineResources in Beta?  ","permalink":"https://cloudnative.to/blog/how-tekton-works/","tags":["Tekton","CICD"],"title":"Tekton 的工作原理"},{"categories":["Kubernetes"],"contents":" 接触 Kubernetes 以来，我经常看到 Kubernetes 在不同的地方使用了证书（Certificate），在 Kubernetes 安装和组件启动参数中也需要配置大量证书相关的参数。但是 Kubernetes 的文档在解释这些证书的工作机制方面做得并不是太好。经过大量的相关阅读和分析工作后，我基本弄清楚了 Kubernetes 中证书的使用方式。在本文中，我将试图以一种比官方文档更容易理解的方式来说明 Kubernetes 证书相关的工作机制，如果你也存在这方面的疑惑，希望这篇文章对你有所帮助。\nKubernetes 组件的认证方式 首先让我们来看一下 Kubernetes 中的组件：在 Kubernetes 中包含多个以独立进程形式运行的组件，这些组件之间通过 HTTP/gRPC 相互通信，以协同完成集群中应用的部署和管理工作。\nkubernetes 组件，图片来源kubernetes.io\n从图中可以看到，Kubernetes 控制平面中包含了 etctd，kube-api-server，kube-scheduler，kube-controller-manager 等组件，这些组件会相互进行远程调用，例如 kube-api-server 会调用 etcd 接口存储数据，kube-controller-manager 会调用 kube-api-server 接口查询集群中的对象状态；同时，kube-api-server 也会和在工作节点上的 kubelet 和 kube-proxy 进行通信，以在工作节点上部署和管理应用。\n以上这些组件之间的相互调用都是通过网络进行的。在进行网络通信时，通信双方需要验证对方的身份，以避免恶意第三方伪造身份窃取信息或者对系统进行攻击。为了相互验证对方的身份，通信双方中的任何一方都需要做下面两件事情：\n 向对方提供标明自己身份的一个证书 验证对方提供的身份证书是否合法，是否伪造的？  在 Kubernetes 中使用了数字证书来提供身份证明，我们可以把数字证书简单理解为我们在日常生活中使用的“身份证”，上面标注了证书拥有者的身份信息，例如名称，所属组织机构等。为了保证证书的权威性，会采用一个通信双方都信任的 CA（证书机构，Certificate Authority）来颁发证书。这就类似于现实生活中颁发“身份证”的政府机构。数字证书中最重要的内容实际上是证书拥有者的公钥，该公钥代表了用户的身份。本文假设读者已经了解数字证书和 CA 的基本原理，如果你对此不太清楚，或者希望重新温习一下相关知识，可以先阅读一下这篇文章《数字证书原理》。\nCA （证书机构），图片来源www.trustauth.cn\n在 Kubernetes 的组件之间进行通信时，数字证书的验证是在协议层面通过 TLS 完成的，除了需要在建立通信时提供相关的证书和密钥外，在应用层面并不需要进行特殊处理。采用 TLS 进行验证有两种方式：\n 服务器单向认证：只需要服务器端提供证书，客户端通过服务器端证书验证服务的身份，但服务器并不验证客户端的身份。这种情况一般适用于对 Internet 开放的服务，例如搜索引擎网站，任何客户端都可以连接到服务器上进行访问，但客户端需要验证服务器的身份，以避免连接到伪造的恶意服务器。 双向 TLS 认证：除了客户端需要验证服务器的证书，服务器也要通过客户端证书验证客户端的身份。这种情况下服务器提供的是敏感信息，只允许特定身份的客户端访问。  在 Kubernetes 中，各个组件提供的接口中包含了集群的内部信息。如果这些接口被非法访问，将影响集群的安全，因此组件之间的通信需要采用双向 TLS 认证。即客户端和服务器端都需要验证对方的身份信息。在两个组件进行双向认证时，会涉及到下面这些证书相关的文件：\n 服务器端证书：服务器用于证明自身身份的数字证书，里面主要包含了服务器端的公钥以及服务器的身份信息。 服务器端私钥：服务器端证书中包含的公钥所对应的私钥。公钥和私钥是成对使用的，在进行 TLS 验证时，服务器使用该私钥来向客户端证明自己是服务器端证书的拥有者。 客户端证书：客户端用于证明自身身份的数字证书，里面主要包含了客户端的公钥以及客户端的身份信息。 客户端私钥：客户端证书中包含的公钥所对应的私钥，同理，客户端使用该私钥来向服务器端证明自己是客户端证书的拥有者。 服务器端 CA 根证书：签发服务器端证书的 CA 根证书，客户端使用该 CA 根证书来验证服务器端证书的合法性。 客户端端 CA 根证书：签发客户端证书的 CA 根证书，服务器端使用该 CA 根证书来验证客户端证书的合法性。  下面这张来自The magic of TLS, X509 and mutual authentication explained 文章中的图形象地解释了双向 TLS 认证的原理。如果你需要了解更多关于 TLS 认证的原理，可以阅读一下 medium 上的原文。\n图片来源The magic of TLS, X509 and mutual authentication explained\nKubernetes 中使用到的CA和证书 Kubernetes 中使用了大量的证书，本文不会试图覆盖到所有可能使用到的证书，但会讨论到主要的证书。理解了这些证书的使用方法和原理后，也能很快理解其他可能遇到的证书文件。下图标识出了在 kubernetes 中主要使用到的证书和其使用的位置：\nKubernetes 中使用到的主要证书\n上图中使用序号对证书进行了标注。图中的箭头表明了组件的调用方向，箭头所指方向为服务提供方，另一头为服务调用方。为了实现 TLS 双向认证，服务提供方需要使用一个服务器证书，服务调用方则需要提供一个客户端证书，并且双方都需要使用一个 CA 证书来验证对方提供的证书。为了简明起见，上图中只标注了证书使用方提供的证书，并没有标注证书的验证方验证使用的 CA 证书。图中标注的这些证书的作用分别如下：\n etcd 集群中各个节点之间相互通信使用的证书。由于一个 etctd 节点既为其他节点提供服务，又需要作为客户端访问其他节点，因此该证书同时用作服务器证书和客户端证书。\n etcd 集群向外提供服务使用的证书。该证书是服务器证书。\n kube-apiserver 作为客户端访问 etcd 使用的证书。该证书是客户端证书。\n kube-apiserver 对外提供服务使用的证书。该证书是服务器证书。\n kube-controller-manager 作为客户端访问 kube-apiserver 使用的证书，该证书是客户端证书。\n kube-scheduler 作为客户端访问 kube-apiserver 使用的证书，该证书是客户端证书。\n kube-proxy 作为客户端访问 kube-apiserver 使用的证书，该证书是客户端证书。\n kubelet 作为客户端访问 kube-apiserver 使用的证书，该证书是客户端证书。\n 管理员用户通过 kubectl 访问 kube-apiserver 使用的证书，该证书是客户端证书。\n kubelet 对外提供服务使用的证书。该证书是服务器证书。\n kube-apiserver 作为客户端访问 kubelet 采用的证书。该证书是客户端证书。\n kube-controller-manager 用于生成和验证 service-account token 的证书。该证书并不会像其他证书一样用于身份认证，而是将证书中的公钥/私钥对用于 service account token 的生成和验证。kube-controller-manager 会用该证书的私钥来生成 service account token，然后以 secret 的方式加载到 pod 中。pod 中的应用可以使用该 token 来访问 kube-apiserver， kube-apiserver 会使用该证书中的公钥来验证请求中的 token。我们将在文中稍后部分详细介绍该证书的使用方法。\n  通过这张图，对证书机制比较了解的读者可能已经看出，我们其实可以使用多个不同的 CA 来颁发这些证书。只要在通信的组件中正确配置用于验证对方证书的 CA 根证书，就可以使用不同的 CA 来颁发不同用途的证书。但我们一般建议采用统一的 CA 来颁发 kubernetes 集群中的所有证书，这是因为采用一个集群根 CA 的方式比采用多个 CA 的方式更容易管理，可以避免多个CA 导致的复杂的证书配置、更新等问题，减少由于证书配置错误导致的集群故障。\nKubernetes 中的证书配置 前面我们介绍了 Kubernetes 集群中主要使用到的证书。下面我们分别看一下如何将这些证书及其对应的私钥和 CA 根证书需要配置到 Kubernetes 中各个组件中，以供各个组件进行使用。这里假设使用一个集群根 CA 来颁发所有相关证书，因此涉及到 CA 的配置对应的证书文件名都是相同的。\netcd 证书配置 需要在 etcd 的启动命令行中配置以下证书相关参数：\n etcd 对外提供服务的服务器证书及私钥。 etcd 节点之间相互进行认证的 peer 证书、私钥以及验证 peer 的 CA。 etcd 验证访问其服务的客户端的 CA。  /usr/local/bin/etcd \\\\ --cert-file=/etc/etcd/kube-etcd.pem \\\\ # 对外提供服务的服务器证书 --key-file=/etc/etcd/kube-etcd-key.pem \\\\ # 服务器证书对应的私钥 --peer-cert-file=/etc/etcd/kube-etcd-peer.pem \\\\ # peer 证书，用于 etcd 节点之间的相互访问 --peer-key-file=/etc/etcd/kube-etcd-peer-key.pem \\\\ # peer 证书对应的私钥 --trusted-ca-file=/etc/etcd/cluster-root-ca.pem \\\\ # 用于验证访问 etcd 服务器的客户端证书的 CA 根证书 --peer-trusted-ca-file=/etc/etcd/cluster-root-ca.pem\\\\ # 用于验证 peer 证书的 CA 根证书 ...  kube-apiserver 证书配置 需要在 kube-apiserver 中配置以下证书相关参数：\n kube-apiserver 对外提供服务的服务器证书及私钥。 kube-apiserver 访问 etcd 所需的客户端证书及私钥。 kube-apiserver 访问 kubelet 所需的客户端证书及私钥。 验证访问其服务的客户端的 CA。 验证 etcd 服务器证书的 CA 根证书。 验证 service account token 的公钥。  /usr/local/bin/kube-apiserver \\\\ --tls-cert-file=/var/lib/kubernetes/kube-apiserver.pem \\\\ # 用于对外提供服务的服务器证书 --tls-private-key-file=/var/lib/kubernetes/kube-apiserver-key.pem \\\\ # 服务器证书对应的私钥 --etcd-certfile=/var/lib/kubernetes/kube-apiserver-etcd-client.pem \\\\ # 用于访问 etcd 的客户端证书 --etcd-keyfile=/var/lib/kubernetes/kube-apiserver-etcd-client-key.pem \\\\ # 用于访问 etcd 的客户端证书的私钥 --kubelet-client-certificate=/var/lib/kubernetes/kube-apiserver-kubelet-client.pem \\\\ # 用于访问 kubelet 的客户端证书 --kubelet-client-key=/var/lib/kubernetes/kube-apiserver-kubelet-client-key.pem \\\\ # 用于访问 kubelet 的客户端证书的私钥 --client-ca-file=/var/lib/kubernetes/cluster-root-ca.pem \\\\ # 用于验证访问 kube-apiserver 的客户端的证书的 CA 根证书 --etcd-cafile=/var/lib/kubernetes/cluster-root-ca.pem \\\\ # 用于验证 etcd 服务器证书的 CA 根证书 --kubelet-certificate-authority=/var/lib/kubernetes/cluster-root-ca.pem \\\\ # 用于验证 kubelet 服务器证书的 CA 根证书 --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ # 用于验证 service account token 的公钥 ...  采用 kubeconfig 访问 kube-apiserver Kubernetes 中的各个组件，包括kube-controller-mananger、kube-scheduler、kube-proxy、kubelet等，采用一个kubeconfig 文件中配置的信息来访问 kube-apiserver。该文件中包含了 kube-apiserver 的地址，验证 kube-apiserver 服务器证书的 CA 证书，自己的客户端证书和私钥等访问信息。\n在一个使用 minikube 安装的集群中，生成的 kubeconfig 配置文件如下所示，这四个文件分别为 admin 用户， kube-controller-mananger、kubelet 和 kube-scheduler 的kubeconfig配置文件。\n$ ls /etc/kubernetes/ admin.conf controller-manager.conf kubelet.conf scheduler.conf  我们打开 controller-manager.conf 来看一下，为了节约篇幅，这里没有写出证书和私钥的完整内容。\napiVersion: v1 clusters: - cluster: # 用于验证 kube-apiserver 服务器证书的 CA 根证书 certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX0tLS0tCg== server: https://localhost:8443 name: kubernetes contexts: - context: cluster: kubernetes user: system:kube-controller-manager name: system:kube-controller-manager@kubernetes current-context: system:kube-controller-manager@kubernetes kind: Config preferences: {} users: - name: system:kube-controller-manager user: # 用于访问 kube-apiserver 的客户端证书 client-certificate-data: LS0tLS1CRUdJTiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXQ0FURS0tLS0tCg== # 客户端证书对应的私钥 client-key-data: LS0tLS1CRUdXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXtFWS0tLS0tCg==  可以看到，访问 kube-apiserver 所需要的相关证书内容已经被采用 base64 编码写入了文件中。其他几个文件中的内容也是类似的，只是配置的用户名和客户端证书有所不同。\n在启动这些组件时，需要在参数中指出 kubeconfig 文件的路径，例如 kube-controller-manager 的启动命令如下。\n/usr/local/bin/kube-controller-manager \\\\ --kubeconfig=/etc/kubernetes/controller-manager.conf # 下面几个证书和访问 kube-apiserver 无关，我们会在后面介绍到 --cluster-signing-cert-file=/var/lib/kubernetes/cluster-root-ca.pem # 用于签发证书的 CA 根证书 --cluster-signing-key-file=/var/lib/kubernetes/cluster-root-ca-key.pem # 用于签发证书的 CA 根证书的私钥 --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem # 用于对 service account token 进行签名的私钥 ...  Service Account 证书 Kubernetes 中有两类用户，一类为 user account，一类为 service account。 service account 主要被 pod 用于访问 kube-apiserver。 在为一个 pod 指定了 service account 后，kubernetes 会为该 service account 生成一个 JWT token，并使用 secret 将该 service account token 加载到 pod 上。pod 中的应用可以使用 service account token 来访问 api server。service account 证书被用于生成和验证 service account token。该证书的用法和前面介绍的其他证书不同，因为实际上使用的是其公钥和私钥，而并不需要对证书进行验证。\n我们可以看到 service account 证书的公钥和私钥分别被配置到了 kube-apiserver 和 kube-controller-manager 的命令行参数中，如下所示：\n/usr/local/bin/kube-apiserver \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ # 用于验证 service account token 的公钥 ... /usr/local/bin/kube-controller-manager \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem # 用于对 service account token 进行签名的私钥 ...  下图展示了 kubernetes 中生成、使用和验证 service account token 的过程。\n认证方法：客户端证书还是 token ？ 我们可以看到，Kubernetes 提供了两种客户端认证的方法，控制面组件采用的是客户端数字证书;而在集群中部署的应用则采用了 service account token 的方式。为什么 Kubernetes 不为 service account 也生成一个证书，并采用该证书进行身份认证呢？ 实际上 Istio 就是这样做的，Istio 会自动为每个 service account 生成一个证书，并使用该证书来在 pod 中的应用之间建立双向 tls 认证。我没有找到 Kubernetes 这个设计决策的相关说明，如果你知道原因或对此有自己的见解，欢迎联系我进行探讨。\nKubernetes 证书签发 Kubernetes 提供了一个 certificates.k8s.io API，可以使用配置的 CA 根证书来签发用户证书。该 API 由 kube-controller-manager 实现，其签发证书使用的根证书在下面的命令行中进行配置。我们希望 Kubernetes 采用集群根 CA 来签发用户证书，因此在 kube-controller-manager 的命令行参数中将相关参数配置为了集群根 CA。\n/usr/local/bin/kube-controller-manager \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/cluster-root-ca.pem # 用于签发证书的 CA 根证书 --cluster-signing-key-file=/var/lib/kubernetes/cluster-root-ca-key.pem # 用于签发证书的 CA 根证书的私钥 ...  关于更多 Kubernetes 证书签发 API 的内容，可以参见 管理集群中的 TLS 认证。\n使用 TLS bootstrapping 简化 Kubelet 证书制作 在安装 Kubernetes 时，我们需要为每一个工作节点上的 Kubelet 分别生成一个证书。由于工作节点可能很多，手动生成 Kubelet 证书的过程会比较繁琐。为了解决这个问题，Kubernetes 提供了 TLS bootstrapping  的方式来简化 Kubelet 证书的生成过程。其原理是预先提供一个 bootstrapping token，kubelet 通过该 kubelet 调用 kube-apiserver 的证书签发 API 来生成 自己需要的证书。要启用该功能，需要在 kube-apiserver 中启用 --enable-bootstrap-token-auth ，并创建一个 kubelet 访问 kube-apiserver 使用的 bootstrap token secret。如果使用 kubeadmin 安装，可以使用 kubeadm token create命令来创建 token。\n采用TLS bootstrapping 生成证书的流程如下：\n 调用 kube-apiserver 生成一个 bootstrap token。 将该 bootstrap token 写入到一个 kubeconfig 文件中，作为 kubelet 调用 kube-apiserver 的客户端验证方式。 通过 --bootstrap-kubeconfig 启动参数将 bootstrap token 传递给 kubelet 进程。 Kubelet 采用bootstrap token 调用 kube-apiserver API，生成自己所需的服务器和客户端证书。 证书生成后，Kubelet 采用生成的证书和 kube-apiserver 进行通信，并删除本地的 kubeconfig 文件，以避免 bootstrap token 泄漏风险。  小结 Kubernetes 中使用了大量的证书来确保集群的安全，弄清楚这些证书的用途和配置方法将有助于我们深入理解 kubernetes 的安装过程和组件的配置。本文是笔者在学习 过程中整理的 Kubernetes 集群中主要使用到的证书，由于笔者对 Kubernetes 的理解有限，文章中难免存在部分错误，欢迎指正。\n参考文档  Kubernetes PKI 证书和要求 kubernetes the hard way Kubernetes 之 二进制安装(二) 证书详解 TLS bootstrapping 数字证书原理  ","permalink":"https://cloudnative.to/blog/k8s-certificate/","tags":["Kubernetes"],"title":"一文带你彻底厘清 Kubernetes 中的证书工作机制"},{"categories":["Service Mesh"],"contents":" 前言 Service Mesh 在企业落地中有诸多挑战，当与传统微服务应用共同部署治理时可用性挑战更为严峻。本文将以 Service Mesh 与 Spring Cloud 应用互联互通共同治理为前提，着重介绍基于 Consul 的注册中心高可用方案，通过各种限流、熔断策略保证后端服务的高可用，以及通过智能路由策略（负载均衡、实例容错等）实现服务间调用的高可用。\nService Mesh 与 Spring Cloud 应用的互通、互联 微服务是时下技术热点，大量互联网公司都在做微服务架构的推广和落地。同时，也有很多传统企业基于微服务和容器，在做互联网技术转型。而在这个技术转型中，国内有一个现象，以 Spring Cloud 与 Dubbo 为代表的微服务开发框架非常普及和受欢迎。近年来， 新兴的 Service Mesh 技术也越来越火热，受到越来越多开发者的关注，大有后来居上的趋势。\n在听到社区里很多人谈到微服务技术选型时，注意到他们讨论一个非此即彼的问题：采用 Spring Cloud 还是以 Istio 为代表的 Service Mesh 技术？然而这个答案并非非黑即白、非你即我，一部分应用采用 Spring Cloud，另一部分采用 Service Mesh（Istio）是完全可能的。今天我就和大家一起来讨论这个问题。\n首先，我们来看一下 Spring Cloud 这个传统侵入式微服务框架。它包含以下优点：\n 集大成者，Spring Cloud 包含了微服务架构的方方面面；选用目前各家公司开发的比较成熟的、经得住实践考验的服务框架； 轻量级组件，Spring Cloud 整合的组件大多比较轻量级，且都是各自领域的佼佼者； 开发简便，Spring Cloud 对各个组件进行了大量的封装，从而简化了开发； 开发灵活，Spring Cloud 的组件都是解耦的，开发人员可以灵活按需选择组件。  特别感谢 Netflix ，这家很早就成功实践微服务的公司，几年前把自家几乎整个微服务框架栈贡献给了社区，早期的 Spring Cloud 主要是对 Netflix 开源组件的进一步封装。不过近两年，Spring Cloud 社区开始自研了很多新的组件，也接入了其他一些互联网公司的优秀实践。\n接下来，我们简单看一下 Service Mesh 框架。它带来了两大变革：微服务治理与业务逻辑的解耦，异构系统的统一治理。此外，服务网格相对于传统微服务框架，还拥有三大技术优势：可观察性、流量控制、安全。服务网格带来了巨大变革并且拥有其强大的技术优势，被称为第二代“微服务架构”。\n然而就像之前说的软件开发没有银弹，传统微服务架构有许多痛点，而服务网格也不例外，也有它的局限性。这些局限性包括：增加了链路与运维的复杂度、需要更专业的运维技能、带来了一定的延迟以及对平台的适配。\n更多关于 Spring Cloud 与 Service Mesh 的优缺点与比较，请阅读 Istio-Handbook [Service Mesh 概述]。\n前面提到过，对于传统微服务框架 Spring Cloud 与新兴微服务框架 Service Mesh，并非是个非黑即白，非你即我，延伸到微服务与单体架构，它们也是可以共存的。\n也可以将其与混合云相类比，混合云中包含了公有云、私有云，可能还有其它的自有基础设施。目前来看，混合云是一种流行的实践方式；实际上，可能很难找到一个完全单一云模式的组织。对多数组织来说，将一个单体应用完全重构为微服务的过程中，对开发资源的调动是一个很严峻的问题；采用混合微服务策略是一个较好的方式，对开发团队来说，这种方式让微服务架构触手可及；否则的话，开发团队可能会因为时间、经验等方面的欠缺，无法接受对单体应用的重构工作。\n构建混合微服务架构的最佳实践：\n 最大化收益的部分优先重构； 非 Java 应用优先采用 Service Mesh 框架。  混合微服务出现的原因是为了更好的支持平滑迁移，最大限度的提升服务治理水平，降低运维通信成本等，并且可能会在一个较长的周期存在着。而实现这一架构的前提，就是各服务的“互联互通”。\n要想实现上述“混合微服务架构”，运行时支撑服务必不可少，它主要包括服务注册中心、服务网关和集中式配置中心三个产品。\n传统微服务和 Service Mesh 双剑合璧（双模微服务），即“基于 SDK 的传统微服务”可以和“基于 Sidecar 的 Service Mesh 微服务”实现下列目标：\n 互联互通：两个体系中的应用可以相互访问； 平滑迁移：应用可以在两个体系中迁移，对于调用该应用的其他应用，做到透明无感知； 灵活演进：在互联互通和平滑迁移实现之后，我们就可以根据实际情况进行灵活的应用改造和架构演进。  这里还包括对应用运行平台的要求，即两个体系下的应用，既可以运行在虚拟机之上，也可以运行在容器 /K8s 之上。我们不希望把用户绑定在 K8s 上，因此 Service Mesh 没有采用 K8s 的 Service 机制来做服务注册与发现，这里就突出了注册中心的重要性。\n百度智能云 CNAP 团队实现了上述混合微服务架构，即实现了两个微服务体系的应用互联互通、平滑迁移、灵活演进。上述混合微服务架构图包括以下几个组件：\n API Server：前后端解耦，接口权限控制、请求转发、异常本地化处理等等； 微服务控制中心：微服务治理的主要逻辑，包括服务注册的多租户处理、治理规则（路由、限流、熔断）的创建和转换、微服务配置的管理； 监控数据存储、消息队列：主要是基于 Trace 的监控方案使用的组件； 配置中心：微服务配置中心，最主要的功能是支持配置管理，包括治理规则、用户配置等所有微服务配置的存储和下发，微服务配置中心的特色是借助 SDK 可以实现配置/规则热更新。  接下来主要看一下注册中心的服务注册和发现机制：\n Spring Cloud 应用通过 SDK、Service Mesh 应用实现 Sidecar 分别向注册中心注册，注册的请求先通过微服务控制中心进行认证处理与多租户隔离； Mesh 控制面直接对接注册中心获取服务实例、Spring Cloud 应用通过 SDK 获取服务实例； 双模异构，支持容器与虚机两种模型。  注册中心与高可用方案 前面提到过，要想实现实现混合微服务架构，注册中心很关键。谈到注册中心，目前主流的开源注册中心包括：\n Zookeeper：Yahoo 公司开发的分布式协调系统，可用于注册中心，目前仍有很多公司使用其作为注册中心； Eureka：Netflix 开源组件，可用于服务注册发现组件，被广大 Spring Cloud 开发者熟知，遗憾的是目前已经不再维护，也不再被 Spring Cloud 生态推荐使用； Consul： HashiCorp 公司推出的产品，其可作为实现注册中心，也是本文介绍的重点； Etcd：Etcd 官方将其定义为可靠的分布式 KV 存储。  我们注册中心选择了 Consul，Consul 包含了以下几个重要的功能：\n 服务发现：可以注册服务，也可以通过 Http 或 DNS 的方式发现已经注册的服务； 丰富的健康检查机制； 服务网格能力，最新版本已经支持 Envoy 作为数据面； KV 存储：可以基于 Consul KV 存储实现一个分布式配置中心； 多数据中心：借助多数据中心，无需使用额外的抽象层，即可构建多地域的场景，支持多 DC 数据同步、异地容灾。  上图是 Consul 官网提供的架构图。Consul 架构中几个核心的概念如下：\n Agent: Agent 是运行在 Consul 集群的每个节点上的 Daemon 进程，通过 Consul Agent 命令将其启动，Agent 可以运行在 Client 或者 Server 模式下； Client：Client 是一种 Agent，其将会重定向所有的 RPC 请求到 Server，Client 是无状态的，其主要参与 LAN Gossip 协议池，其占用很少的资源，并且消耗很少的网络带宽； Server：Server 是一种 Agent，其包含了一系列的责任包括：参与 Raft 协议写半数（Raft Quorum）、维护集群状态、响应 RPC 响应、和其他 Datacenter 通过 WAN gossip 交换信息和重定向查询请求至 Leader 或者远端 Datacenter； Datacenter: Datacenter 其是私有的、低延迟、高带宽的网络环境，去除了在公共网络上的网络交互。  注册中心作为基础组件，其自身的可用性显得尤为重要，高可用的设计需要对其进行分布式部署，同时因在分布式环境下的复杂性，节点因各种原因都有可能发生故障，因此在分布式集群部署中，希望在部分节点故障时，集群依然能够正常对外服务。注册中心作为微服务基础设施，因此对其容灾和其健壮性有一定的要求，主要体现在：\n 注册中心作为微服务基础设施，因此要求出现某些故障（如节点挂掉、网络分区）后注册中心仍然能够正常运行； 当注册中心的发生故障时，不能影响服务间的正常调用。  Consul 使用 Raft 协议作为其分布式一致性协议，本身对故障节点有一定的容忍性，在单个 DataCenter中 Consul 集群中节点的数量控制在 2*n + 1 个节点，其中 n 为可容忍的宕机个数。Quorum size: Raft 协议选举需要半数以上节点写入成功。\nQ1: 节点的个数是否可以为偶数个？\nA2：答案是可以的，但是不建议部署偶数个节点。一方面如上表中偶数节点4和奇数节点3可容忍的故障数是一样的，另一方面，偶数个节点在选主节点的时候可能会出现瓜分选票的情形（虽然 Consul 通过重置 election timeout 来重新选举），所以还是建议选取奇数个节点。\nQ2: 是不是 Server 节点个数越多越好？\nA2：答案是否定的，虽然上表中显示 Server 数量越多可容忍的故障数越多，熟悉 Raft 协议的读者肯定熟悉 Log Replication（ 如上文介绍，日志复制时过半写成功才返回写成功），随着 Server 的数量越来越多，性能就会越低，所以结合实际场景一般建议 Server 部署3个节点。\n推荐采用三节点或五节点，最为有效，且能容错。\n注册中心设计的一个重要前提是：注册中心不能因为自身的原因或故障影响服务之间的相互调用。因此在实践过程中，如果注册中心本身发生了宕机故障/不可用，绝对不能影响服务之间的调用。这要求对接注册中心的 SDK 针对这种特殊情况进行客户端容灾设计，『客户端缓存』就是一种行之有效的手段。当注册中心发生故障无法提供服务时，服务本身并不会更新本地客户端缓存，利用其已经缓存的服务列表信息，正常完成服务间调用。\n我们在设计时采用同 Datacenter 集群内部部署3个 Server 节点，来保障高可用性，当集群中1个节点发生故障后，集群仍然能够正常运行，同时这3个节点部署在不同的机房，达到机房容灾的能力。\n在云上环境，涉及多 region 环境，因此在架构设计设计时，我们首先将 Consul 的一个 Datacenter 对应云上一个 region，这样更符合 Consul 对于 Datecenter 的定义（DataCenter 数据中心是私有性、低延迟、高带宽的网络环境）。中间代理层实现了服务鉴权、多租户隔离等功能；还可以通过中间代理层，对接多注册中心。\n云上环境存在多租户隔离的需求，即：A租户的服务只能发现A租户服务的实例。针对此场景，需要在 『中间代理层』完成对多租户隔离功能的实现，其主要实践思路为使用 Consul Api Feature 具备 Filtering 功能：\n 利用 Filtering 功能实现租户隔离需求； 减少查询注册中心接口时网络负载。  通过治理策略保证服务高可用 什么是高可用？维基百科这么定义：系统无中断地执行其功能的能力，代表系统的可用性程度，是进行系统设计时的准则之一。我们通常用 N 个9来定义系统的可用性，如果能达到4个9，则说明系统具备自动恢复能力；如果能达到5个9，则说明系统极其健壮，具有极高可用性，而能达到这个指标则是非常难的。\n常见的系统不可用因素包括：程序和配置出 bug、机器故障、机房故障、容量不足、依赖服务出现响应超时等。高可用的抓手包括：研发质量、测试质量、变更管理、监控告警、故障预案、容量规划、放火盲测、值班巡检等。这里，将主要介绍通过借助治理策略采用高可用设计手段来保障高可用。\n高可用是一个比较复杂的命题，所以设计高可用方案也涉及到了方方面面。这中间将会出现的细节是多种多样的，所以我们需要对这样一个微服务高可用方案进行一个顶层的设计。\n比如服务冗余：\n 冗余策略：每个机器每个服务都可能出现问题，所以第一个考虑到的就是每个服务必须不止一份，而是多份。所谓多份一致的服务就是服务的冗余，这里说的服务泛指了机器的服务、容器的服务、还有微服务本身的服务。在机器服务层面需要考虑，各个机器间的冗余是否有在物理空间进行隔离冗余。 无状态化：我们可以随时对服务进行扩容或者缩容，想要对服务进行随时随地的扩缩容，就要求我们的服务是一个无状态化，所谓无状态化就是每个服务的服务内容和数据都是一致的。  比如柔性化/异步化：\n 所谓的柔性化，就是在我们业务允许的情况下，做不到给予用户百分百可用的，通过降级的手段给到用户尽可能多的服务，而不是非得每次都交出去要么 100 分或 0 分的答卷。柔性化更多是一种思维，需要对业务场景有深入的了解。 异步化：在每一次调用，时间越长存在超时的风险就越大，逻辑越复杂执行的步骤越多，存在失败的风险也就越大。如果在业务允许的情况下，用户调用只给用户必须要的结果，不是需要同步的结果可以放在另外的地方异步去操作，这就减少了超时的风险也把复杂业务进行拆分减低复杂度。  上面讲到的几种提高服务高可用的手段，大多需要从业务以及部署运维的角度实现。而接下来会重点介绍，可以通过 SDK/Sidecar 手段提供服务高可用的治理策略，这些策略往往对业务是非侵入或者弱侵入的，能够让绝大多数服务轻松实现服务高可用。\n微服务之间一旦建立起路由，就意味着会有数据在服务之间流通。由于不同服务可以提供的资源和对数据流量的承载能力不尽相同，为了防止单个 Consumer 占用 Provider 过多的资源，或者突发的大流量冲击导致 Provider 故障，需要服务限流来保证服务的高可用。\n在服务治理中，虽然我们可以通过限流规则尽量避免服务承受过高的流量，但是在实际生产中服务故障依然难以完全避免。当整个系统中某些服务产生故障时，如果不及时采取措施，这种故障就有可能因为服务之间的互相访问而被传播开来，最终导致故障规模的扩大，甚至导致整个系统奔溃，这种现象我们称之为“雪崩”。熔断降级其实不只是服务治理中，在金融行业也有很广泛的应用。比如当股指的波动幅度超过规定的熔断点时，交易所为了控制风险采取的暂停交易措施。\n负载均衡是高可用架构的一个关键组件，主要用来提高性能和可用性，通过负载均衡将流量分发到多个服务器，同时多服务器能够消除这部分的单点故障。\n以上治理规则在某种程度上可以在 Spring Cloud 与 Service Mesh 两个框架上进行对齐，即同一套治理配置，可以通过转换分发到 Spring Cloud 应用的 SDK 上以及 Service Mesh 的 Sidecar 上。可以由 Config-server 负责规则下发，也可以由 Service Mesh 的控制面负责下发，取决于具体的架构方案。\n服务限流 对于一个应用系统来说一定会有极限并发/请求数，即总有一个 TPS/QPS 阀值，如果超了阀值则系统就会不响应用户请求或响应的非常慢，因此我们最好进行过载保护，防止大量请求涌入击垮系统。限流的目的是通过对并发访问/请求进行限速或者一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务或进行流量整形。\n常用的微服务限流架构包括：\n 接入层（api-gateway）限流：  单实例； 多实例：分布式限流算法；  调用外部限流服务限流：  微服务收到请求后，通过限流服务暴露的 RPC 接口查询是否超过阈值； 需单独部署限流服务；  切面层限流（SDK）：  限流功能集成在微服务系统切面层，与业务解耦； 可结合远程配置中心使用；   常用的限流策略包括：\n 拒绝策略：  超过阈值直接返回错误； 调用方可做熔断降级处理。  延迟处理：  前端设置一个流量缓冲池，将所有的请求全部缓冲进这个池子，不立即处理。然后后端真正的业务处理程序从这个池子中取出请求依次处理，常见的可以用队列模式来实现（MQ：削峰填谷）； 用异步的方式去减少了后端的处理压力。  特权处理：  这个模式需要将用户进行分类，通过预设的分类，让系统优先处理需要高保障的用户群体，其它用户群的请求就会延迟处理或者直接不处理。   常用的限流算法包括：\n 固定时间窗口限流：  首先需要选定一个时间起点，之后每次接口请求到来都累加计数器，如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许 100 次接口请求），累加访问次数超过限流值，则限流熔断拒绝接口请求。当进入下一个时间窗口之后，计数器清零重新计数； 缺点在于：限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。  滑动时间窗口算法：\n 流量经过滑动时间窗口算法整形之后，可以保证任意时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑，可以部分解决上面提到的临界突发流量问题，是对固定时间窗口算法的一种改进； 缺点在于：需要记录在时间窗口内每个接口请求到达的时间点，对内存的占用会比较多。  令牌桶算法：\n 接口限制 t 秒内最大访问次数为 n，则每隔 t/n 秒会放一个 token 到桶中； 桶中最多可以存放 b 个 token，如果 token 到达时令牌桶已经满了，那么这个 token 会被丢弃； 接口请求会先从令牌桶中取 token，拿到 token 则处理接口请求，拿不到 token 就阻塞或者拒绝服务。  漏桶算法：\n 对于取令牌的频率也有限制，要按照 t/n 固定的速度来取令牌； 实现往往依赖于队列，请求到达如果队列未满则直接放入队列，然后有一个处理器按照固定频率从队列头取出请求进行处理。如果请求量大，则会导致队列满，那么新来的请求就会被抛弃； 令牌桶和漏桶算法的算法思想大体类似，漏桶算法作为令牌桶限流算法的改进版本。   令牌桶算法和漏桶算法，在某些场景下（内存消耗、应对突发流量），这两种算法会优于时间窗口算法成为首选。\n熔断 断路器模式是微服务架构中广泛采用的模式之一，旨在将故障的影响降到最低，防止级联故障和雪崩，并确保端到端性能。我们将比较使用两种不同方法实现它的优缺点: Hystrix 和 Istio。\n在电路领域中，断路器是为保护电路而设计的一种自动操作的电气开关。它的基本功能是在检测到故障后中断电流，然后可以重置(手动或自动)，以在故障解决后恢复正常操作。这看起来与我们的问题非常相似：为了保护应用程序不受过多请求的影响，最好在后端检测到重复出现的错误时立即中断前端和后端之间的通信。Michael Nygard 在他的《Release It》一书中使用了这个类比，并为应用于上述超时问题的设计模式提供了一个典型案例，可以用上图来总结。\nIstio 通过 DestinationRule 实现断路器模式，或者更具体的路径 TrafficPolicy (原断路器) -\u0026gt; OutlierDetection，根据上图模型：\n consecutiveErrors 断路器打开前的出错次数； interval 断路器检查分析的时间间隔； baseEjectionTime 最小的开放时间，该电路将保持一段时间等于最小弹射持续时间和电路已打开的次数的乘积； maxEjectionPercent 可以弹出的上游服务的负载平衡池中主机的最大百分比，如果驱逐的主机数量超过阈值，则主机不会被驱逐。  与上述公称断路器相比，有两个主要偏差:\n 没有半开放的状态。然而，断路器持续打开的时间取决于被调用服务之前失败的次数，持续的故障服务将导致断路器的开路时间越来越长。 在基本模式中，只有一个被调用的应用程序(后端)。在更实际的生产环境中，负载均衡器后面可能部署同一个应用程序的多个实例。某些情况下有些实例可能会失败，而有些实例可能会工作。因为 Istio 也有负载均衡器的功能，能够追踪失败的实例，并把它们从负载均衡池中移除，在一定程度上: ‘maxEjectionPercent’ 属性的作用是保持一小部分的实例池。  Hystrix 提供了一个断路器实现，允许在电路打开时执行 fallback 机制。最关键的地方就在 HystrixCommand 的方法 run() 和 getFallback()：\n run() 是要实际执行的代码 e.g. 从报价服务中获取价格； getFallback() 获取当断路器打开时的 fallback 结果 e.g. 返回缓存的价格。  Spring Cloud 是建立在 Spring Boot 之上的框架，它提供了与 Spring 的良好集成。它让开发者在处理 Hystrix 命令对象的实例化时，只需注释所需的 fallback 方法。\n实现断路器的方法有两种，一种是黑盒方式，另一种是白盒方式。Istio 作为一种代理管理工具，使用了黑盒方式，它实现起来很简单，不依赖于底层技术栈，而且可以在事后配置。另一方面，Hystrix 库使用白盒方式，它允许所有不同类型的 fallback:\n 单个默认值； 一个缓存； 调用其他服务。  它还提供了级联回退（cascading fallbacks）。这些额外的特性是有代价的：它需要在开发阶段就做出fallback 的决策。\n这两种方法之间的最佳匹配可能会依靠自己的上下文: 在某些情况下，如引用的服务，一个白盒战略后备可能是一个更好的选择，而对于其他情况下快速失败可能是完全可以接受的，如一个集中的远程登录服务。\n常用的熔断方法包括自动熔断与手动熔断。发生熔断时也可以选择 fail-fast 或者 fallback。这些用户都可以基于需求灵活使用。\n智能路由 最后，我们来看一下智能路由带来的高可用。智能路由这里包括（客户端）负载均衡与实例容错策略。对于 Spring Cloud 框架来说，这部分能力由 Ribbon 来提供，Ribbon 支持随机、轮询、响应时间权重等负载均衡算法。而对于 Service Mesh 框架，这部分能力由 Envoy 提供，Envoy 支持随机、轮询（加权）、环哈希等算法。为了实现两套系统的规则统一对齐，可以采用其交集。\n而容错策略包括：\n failover：失败后自动切换其他服务器，支持配置重试次数； failfast：失败立即报错，不再重试； failresnd：将失败请求放入缓存队列、异步处理，搭配 failover 使用。  Istio 支持重试策略配置，而 fail-fast 即对应与重试次数为0。\n总结 微服务的高可用是一个复杂的问题，往往需要从多个角度去看，包括：\n 从手段看高可用。主要使用的技术手段是服务和数据的冗余备份和失效转移，一组服务或一组数据都能在多节点上，之间相互备份。当一台机器宕机或出现问题的时候，可以从当前的服务切换到其他可用的服务，不影响系统的可用性，也不会导致数据丢失。 从架构看高可用。保持简单的架构，目前多数网站采用的是比较经典的分层架构，应用层、服务层、数据层。应用层是处理一些业务逻辑，服务层提供一些数据和业务紧密相关服务，数据层负责对数据进行读写。简单的架构可以使应用层，服务层可以保持无状态化进行水平扩展，这个属于计算高可用。同时在做架构设计的时候，也应该考虑 CAP 理论。 从硬件看高可用。首先得确认硬件总是可能坏的，网络总是不稳定的。解决它的方法也是一个服务器不够就来多几个，一个机柜不够就来几个，一个机房不够就来几个。 从软件看高可用。软件的开发不严谨，发布不规范也是导致各种不可用出现，通过控制软件开发过程质量监控，通过测试，预发布，灰度发布等手段也是减少不可用的措施。 从治理看高可用。将服务规范化，事前做好服务分割，做好服务监控，预判不可用的出现，在不可用出现之前发现问题，解决问题。比如在服务上线后，根据经验，配置服务限流规则以及自动熔断规则。  参考资料  Service Mesh 概述 Consul 作为注册中心在云环境的实践与应用 有了这三个锦囊，再也不用担心微服务治理了 一文理解微服务高可用的常用手段 微服务断路器模式实现：Istio vs Hystrix  ","permalink":"https://cloudnative.to/blog/microservices-ha-practice/","tags":["service mesh","Microservices","Spring Cloud"],"title":"混合微服务高可用在企业级生产中的实践"},{"categories":["其他"],"contents":" 2020 年伊始，受新冠疫情影响，全球各地的员工开启了在家办公的模式，因此人与人之间的距离感觉被拉远了。但是云原生圈子里有我们这样一群人，因为一个共同的愿景聚集到了一起，组建了社区管理委员会，并在过去的三个月里利用业余时间，齐心协力完成了社区的筹备工作。今天我们要正式宣布云原生社区正式成立了。\n成立背景  Software is eating the world. —— Marc Andreessen\n “软件正在吞噬这个世界” 已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是“Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云原生技术的持续演进！\n随着云计算的不断发展，云原生技术在全球范围内变得越来越受关注，同时国内社区同学也展现了对云原生技术热爱。近些年中国已经孕育众多的云原生技术爱好者，也有自发组织的一些相关技术交流和 meetup，同时在云原生领域也涌现了众多优秀的开源项目，在这样的背景下，一个有理想，有组织，有温度的云原生社区应运而生。\n关于云原生社区 云原生社区是一个有技术、有温度、有情怀的开源社区。由一群开源的狂热爱好者自发成立，秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。\n关于云原生社区管理委员会介绍请见团队介绍页面。\n加入云原生社区，你将获得：\n 更接近源头的知识资讯 更富有价值的人际网络 更专业个性的咨询解答 更亲近意见领袖的机会 更快速高效的个人成长 更多知识分享曝光机会 更多行业人才挖掘发现  加入社区 云原生社区免费申请加入，关注云原生社区官方微信公众号，在公众号中回复关键词 加入，即可获得申请链接。\n","permalink":"https://cloudnative.to/blog/cnc-announcement/","tags":["社区"],"title":"云原生社区成立"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/art-institute.1/","tags":null,"title":"Art Institute of Chicago"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/art-institute/","tags":null,"title":"Art Institute of Chicago"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/carpe-diem/","tags":null,"title":"Carpe Diem Santorini"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/carryyip/","tags":null,"title":"Carry Yip"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/celebrate-with/","tags":null,"title":"Celebrate with Stoli"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/essential-looks.1/","tags":null,"title":"Essential Looks Trend Report"},{"categories":null,"contents":" Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\nQuia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\nBenifits of service Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n Quality Services Clients Satisfaction Quality Services Clients Satisfaction Quality Services Clients Satisfaction  Business Strategy Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia dese runt mollit anim id est laborum. sed ut perspiciatis unde omnis iste natus error sit voluptatem acusantium.\n Quality Services Clients Satisfaction Quality Services  Analyze your business Quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam.\n","permalink":"https://cloudnative.to/project/essential-looks/","tags":null,"title":"Essential Looks Trend Report"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/zhangliying/","tags":null,"title":"张丽颖"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/cherylhuang/","tags":null,"title":"Cheryl Hung"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/nipengfei/","tags":null,"title":"倪朋飞"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/jimmysong/","tags":null,"title":"宋净超（Jimmy Song）"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/suwei/","tags":null,"title":"粟伟"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/luoguangming/","tags":null,"title":"罗广明"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/dongyitao/","tags":null,"title":"董一韬"},{"categories":null,"contents":"","permalink":"https://cloudnative.to/team/longheng/","tags":null,"title":"龙恒"}]